{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1dbf43a",
   "metadata": {},
   "source": [
    "\n",
    "# main_pipeline.ipynb\n",
    "\n",
    "Pipeline to:\n",
    "1) Identify failure years from `year_trends_top_stress.csv` per crop.  \n",
    "2) Build year-level feature matrices from wide county files.  \n",
    "3) Rank analog years to 2023 (Top-N and Elbow).\n",
    "\n",
    "**Requirements:** `pandas`, `numpy`, `scikit-learn`, `matplotlib`\n",
    "\n",
    "Set the paths in the **Config** cell and run all cells top-to-bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1429456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Config ----\n",
    "BASE = \"PSI Hackathon/Kashish_results\"              # base results dir\n",
    "TRENDS_DIR = \"stress_analysis_v2\"                   # folder with <CROP>/year_trends_top_stress.csv\n",
    "WIDE_DIR_MAP = {\n",
    "    \"SOYBEANS\": \"soybeans/soybean_stage_features_wide_plus.csv\",\n",
    "    \"CORN\":     \"corn/corn_stage_features_wide_plus.csv\",\n",
    "    \"COTTON\":   \"cotton/cotton_stage_features_wide_plus.csv\",\n",
    "    \"PEANUTS\":  \"peanuts/peanut_stage_features_wide_plus.csv\",\n",
    "    \"WHEAT\":    \"wheat/wheat_stage_features_wide_plus.csv\",\n",
    "}\n",
    "TARGET_YEAR = 2023\n",
    "TOP_N = 12\n",
    "OUTDIR = \"PSI Hackathon/Kashish_results/failure_years_and_analogs\"\n",
    "Z_HARD = (-1.0, 1.0)   # (Yield_z <=, Stress_z >=)\n",
    "Z_SOFT = (-0.5, 0.5)   # (Yield_z <=, Stress_z >=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def find_trends_csv(base: Path, trends_dir_name: str, crop: str) -> Path:\n",
    "    p = base / trends_dir_name / crop / \"year_trends_top_stress.csv\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing trends CSV for {crop}: {p}\")\n",
    "    return p\n",
    "\n",
    "def build_year_features_from_wide(wide_csv: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(wide_csv)\n",
    "    if \"Year\" not in df.columns:\n",
    "        raise ValueError(f\"'Year' column not found in {wide_csv}\")\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Year\"]).copy()\n",
    "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "\n",
    "    id_like = {\"Year\",\"COUNTY\",\"Name\",\"Commodity\",\"Data Item\"}\n",
    "    num_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "    feat_cols = [c for c in num_cols if c not in id_like]\n",
    "    if len(feat_cols) == 0:\n",
    "        raise ValueError(f\"No numeric feature columns found in {wide_csv}\")\n",
    "    Y = (df.groupby(\"Year\", as_index=True)[feat_cols].mean().sort_index())\n",
    "    Y.index.name = \"Year\"\n",
    "    return Y\n",
    "\n",
    "def detect_failures(trends_csv: Path, z_hard=(-1.0, 1.0), z_soft=(-0.5, 0.5)):\n",
    "    df = pd.read_csv(trends_csv)\n",
    "    for col in (\"Year\",\"Yield_z\",\"Stress_z\"):\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"{trends_csv} missing column: {col}\")\n",
    "    hard = df[(df[\"Yield_z\"] <= z_hard[0]) & (df[\"Stress_z\"] >= z_hard[1])][\"Year\"].astype(int).tolist()\n",
    "    soft = df[(df[\"Yield_z\"] <= z_soft[0])  & (df[\"Stress_z\"] >= z_soft[1])][\"Year\"].astype(int).tolist()\n",
    "    return soft, hard, df\n",
    "\n",
    "def plot_trends(df: pd.DataFrame, crop: str, fail_soft, fail_hard, out_png: Path):\n",
    "    plt.figure(figsize=(9,4.5))\n",
    "    plt.plot(df[\"Year\"], df[\"Yield_z\"], label=\"Yield (z)\")\n",
    "    plt.plot(df[\"Year\"], df[\"Stress_z\"], label=\"Top-stress (z)\", linestyle=\"--\")\n",
    "    for y in fail_soft: plt.axvline(int(y), alpha=0.15)\n",
    "    for y in fail_hard: plt.axvline(int(y), alpha=0.35)\n",
    "    plt.axhline(0, linewidth=0.8)\n",
    "    plt.xlabel(\"Year\"); plt.ylabel(\"z-score\")\n",
    "    plt.title(f\"{crop}: Yield vs Top Stress (z)\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200); plt.close()\n",
    "\n",
    "def rank_analogs(Y: pd.DataFrame, target_year: int) -> pd.DataFrame:\n",
    "    if target_year not in Y.index:\n",
    "        raise ValueError(f\"{target_year} not in year features index.\")\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(Y.values)\n",
    "    yrs = Y.index.to_numpy()\n",
    "    idx = np.where(yrs == target_year)[0][0]\n",
    "    sims = cosine_similarity(X[idx:idx+1], X).ravel()\n",
    "    rank = (pd.DataFrame({\"Year\": yrs, \"cosine_sim\": sims})\n",
    "            .sort_values(\"cosine_sim\", ascending=False))\n",
    "    rank = rank[rank[\"Year\"] != target_year].drop_duplicates(subset=[\"Year\"]).reset_index(drop=True)\n",
    "    return rank\n",
    "\n",
    "def elbow_k(rank: pd.DataFrame, top_n: int) -> int:\n",
    "    s = rank[\"cosine_sim\"].to_numpy()\n",
    "    if len(s) < 3:\n",
    "        return min(len(rank), top_n)\n",
    "    drops = np.diff(s)\n",
    "    return int(np.argmin(drops) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base = Path(BASE)\n",
    "outdir = Path(OUTDIR)\n",
    "ensure_dir(outdir)\n",
    "\n",
    "for crop, rel_path in WIDE_DIR_MAP.items():\n",
    "    print(f\"\\n=== {crop} ===\")\n",
    "    trends_csv = find_trends_csv(base, TRENDS_DIR, crop)\n",
    "    soft, hard, trends_df = detect_failures(trends_csv, Z_HARD, Z_SOFT)\n",
    "\n",
    "    # save failure years table with tag\n",
    "    fail_df = pd.DataFrame({\"Year\": sorted(set(soft + hard))})\n",
    "    fail_df[\"tag\"] = fail_df[\"Year\"].apply(lambda y: \"hard\" if y in set(hard) else (\"soft\" if y in set(soft) else \"\"))\n",
    "    fail_csv = outdir / f\"{crop.lower()}_failure_years.csv\"\n",
    "    fail_df.to_csv(fail_csv, index=False)\n",
    "    print(f\"[{crop}] Failures saved -> {fail_csv.name} | hard={hard} | soft={soft}\")\n",
    "\n",
    "    # plot\n",
    "    plot_png = outdir / f\"{crop.lower()}_yield_stress_z_trend.png\"\n",
    "    plot_trends(trends_df, crop, soft, hard, plot_png)\n",
    "    print(f\"[{crop}] Trend plot -> {plot_png.name}\")\n",
    "\n",
    "    # year-level features\n",
    "    wide_csv = base / rel_path\n",
    "    Y = build_year_features_from_wide(wide_csv)\n",
    "    yf_csv = outdir / f\"{crop.lower()}_year_features.csv\"\n",
    "    Y.to_csv(yf_csv)\n",
    "    print(f\"[{crop}] Year features -> {yf_csv.name} ({Y.shape[0]} years Ã— {Y.shape[1]} feats)\")\n",
    "\n",
    "    # analogs\n",
    "    rank = rank_analogs(Y, TARGET_YEAR)\n",
    "    k_elbow = elbow_k(rank, TOP_N)\n",
    "\n",
    "    topN = rank.head(min(TOP_N, len(rank))).copy()\n",
    "    topE = rank.head(min(k_elbow, len(rank))).copy()\n",
    "\n",
    "    out_topN = outdir / f\"{crop.lower()}_analogs_{TARGET_YEAR}_top{len(topN)}.csv\"\n",
    "    out_elb  = outdir / f\"{crop.lower()}_analogs_{TARGET_YEAR}_elbow{len(topE)}.csv\"\n",
    "    topN.to_csv(out_topN, index=False)\n",
    "    topE.to_csv(out_elb, index=False)\n",
    "    print(f\"[{crop}] Saved Top-{len(topN)} -> {out_topN.name} | Elbow({k_elbow}) -> {out_elb.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Post-processing on top of main_pipeline.py outputs:\n",
    "- Load {crop}_year_features.csv and {crop}_failure_years.csv\n",
    "- Rank analog years to TARGET_YEAR (Top-N and Elbow)\n",
    "- Build row-level training masks from a WIDE file\n",
    "- (Optional) export per-crop analog tables and filtered rows\n",
    "\n",
    "Usage:\n",
    "python analysis_pipeline.py \\\n",
    "  --base \"PSI Hackathon/Kashish_results\" \\\n",
    "  --out  \"PSI Hackathon/Kashish_results/failure_years_and_analogs\" \\\n",
    "  --wide_dir_map '{\"SOYBEANS\":\"soybeans/soybean_stage_features_wide_plus.csv\",\"CORN\":\"corn/corn_stage_features_wide_plus.csv\"}' \\\n",
    "  --target_year 2023 --top_n 12 --exclude soft\n",
    "\"\"\"\n",
    "import json\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_year_features(base_out: Path, crop: str) -> pd.DataFrame:\n",
    "    f = base_out / f\"{crop.lower()}_year_features.csv\"\n",
    "    Y = pd.read_csv(f, index_col=0)\n",
    "    if not Y.index.is_unique:\n",
    "        Y = Y.groupby(level=0).mean()\n",
    "    Y.index = Y.index.astype(int)\n",
    "    return Y.sort_index()\n",
    "\n",
    "def load_fail_years(base_out: Path, crop: str, exclude: str) -> set[int]:\n",
    "    f = base_out / f\"{crop.lower()}_failure_years.csv\"\n",
    "    if not f.exists() or exclude == \"none\":\n",
    "        return set()\n",
    "    df = pd.read_csv(f)\n",
    "    yrs = set(pd.to_numeric(df[\"Year\"], errors=\"coerce\").dropna().astype(int))\n",
    "    if \"tag\" not in df.columns or exclude == \"any\":\n",
    "        return yrs\n",
    "    tag = df[\"tag\"].astype(str).str.lower()\n",
    "    if exclude == \"soft\":\n",
    "        return set(pd.to_numeric(df.loc[tag.str.contains(\"soft\"), \"Year\"], errors=\"coerce\").dropna().astype(int))\n",
    "    if exclude == \"hard\":\n",
    "        return set(pd.to_numeric(df.loc[tag.str.contains(\"hard\"), \"Year\"], errors=\"coerce\").dropna().astype(int))\n",
    "    return set()\n",
    "\n",
    "def rank_analogs(Y: pd.DataFrame, target_year: int) -> pd.DataFrame:\n",
    "    assert target_year in Y.index, f\"{target_year} not in year features.\"\n",
    "    scaler = StandardScaler(); X = scaler.fit_transform(Y.values)\n",
    "    years = Y.index.to_numpy()\n",
    "    i = np.where(years == target_year)[0][0]\n",
    "    sims = cosine_similarity(X[i:i+1], X).ravel()\n",
    "    rank = (pd.DataFrame({\"Year\": years, \"cosine_sim\": sims})\n",
    "              .sort_values(\"cosine_sim\", ascending=False))\n",
    "    return rank[rank[\"Year\"] != target_year].drop_duplicates(\"Year\").reset_index(drop=True)\n",
    "\n",
    "def elbow_k(rank: pd.DataFrame, top_n: int) -> int:\n",
    "    s = rank[\"cosine_sim\"].to_numpy()\n",
    "    if len(s) < 3: return min(len(rank), top_n)\n",
    "    drops = np.diff(s); return int(np.argmin(drops) + 1)\n",
    "\n",
    "def build_year_matrix_from_wide(wide_csv: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(wide_csv)\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Year\"]).copy()\n",
    "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "    id_like = {\"Year\",\"COUNTY\",\"Name\",\"Commodity\",\"Data Item\"}\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feat_cols = [c for c in num_cols if c not in id_like]\n",
    "    Y = df.groupby(\"Year\", as_index=True)[feat_cols].mean().sort_index()\n",
    "    Y.index.name = \"Year\"; return Y\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--base\", required=True, help=\"Base dir (e.g., PSI Hackathon/Kashish_results)\")\n",
    "    ap.add_argument(\"--out\", required=True, help=\"Folder with {crop}_year_features.csv and failures\")\n",
    "    ap.add_argument(\"--wide_dir_map\", required=True,\n",
    "                    help='JSON dict crop->relative WIDE csv path (from --base)')\n",
    "    ap.add_argument(\"--target_year\", type=int, default=2023)\n",
    "    ap.add_argument(\"--top_n\", type=int, default=12)\n",
    "    ap.add_argument(\"--exclude\", choices=[\"none\",\"soft\",\"hard\",\"any\"], default=\"none\",\n",
    "                    help=\"Exclude which failure years from analog ranking\")\n",
    "    ap.add_argument(\"--export_rows\", action=\"store_true\",\n",
    "                    help=\"Also export row-level subsets for Top-N and Elbow\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    base = Path(args.base)\n",
    "    out = Path(args.out)\n",
    "    ensure_dir(out)\n",
    "    wide_map = json.loads(args.wide_dir_map)\n",
    "\n",
    "    for crop, rel_path in wide_map.items():\n",
    "        print(f\"\\n=== {crop} ===\")\n",
    "        # 1) Load year features and failures\n",
    "        Y = load_year_features(out, crop)\n",
    "        excl = load_fail_years(out, crop, args.exclude)\n",
    "        # 2) Rank analogs\n",
    "        rank = rank_analogs(Y, args.target_year)\n",
    "        if excl: rank = rank[~rank[\"Year\"].isin(excl)].reset_index(drop=True)\n",
    "        k = elbow_k(rank, args.top_n)\n",
    "        topN = rank.head(min(args.top_n, len(rank))).copy()\n",
    "        topE = rank.head(min(k, len(rank))).copy()\n",
    "        topN.to_csv(out / f\"{crop.lower()}_analogs_{args.target_year}_top{len(topN)}_{args.exclude}.csv\", index=False)\n",
    "        topE.to_csv(out / f\"{crop.lower()}_analogs_{args.target_year}_elbow{len(topE)}_{args.exclude}.csv\", index=False)\n",
    "        print(f\"[{crop}] Top-{len(topN)} saved | Elbow({k}) saved\")\n",
    "\n",
    "        # 3) Optional: export row-level subsets from WIDE for training\n",
    "        if args.export_rows:\n",
    "            wide_csv = base / rel_path\n",
    "            df = pd.read_csv(wide_csv)\n",
    "            df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "            keep_cols = [\"Year\",\"COUNTY\",\"Name\",\"Commodity\",\"Data Item\"]\n",
    "            # TopN rows\n",
    "            yrs_top = set(topN[\"Year\"].tolist()); yrs_elb = set(topE[\"Year\"].tolist())\n",
    "            rows_top = df[df[\"Year\"].isin(yrs_top)].copy()\n",
    "            rows_elb = df[df[\"Year\"].isin(yrs_elb)].copy()\n",
    "            rows_top.to_csv(out / f\"{crop.lower()}_rows_top{len(topN)}_{args.exclude}.csv\", index=False)\n",
    "            rows_elb.to_csv(out / f\"{crop.lower()}_rows_elbow{len(topE)}_{args.exclude}.csv\", index=False)\n",
    "            print(f\"[{crop}] Exported rows: topN={rows_top.shape} elbow={rows_elb.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
