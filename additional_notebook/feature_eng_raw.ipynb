{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3f704703-3780-49bb-b410-4b8564001d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9af3182a-7adf-4107-acf3-e0be1deb406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows daily: 1534100 | years: 1983 - 2024\n",
      "Stage rows: 37800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "# --- unit helpers (must exist) ---\n",
    "f_to_c     = lambda f: (f-32.0)*(5/9)\n",
    "in_to_mm   = lambda x: x*25.4\n",
    "ft_to_m    = lambda x: x*0.3048\n",
    "hpa_to_kpa = lambda x: x*0.1\n",
    "\n",
    "# --- streak helper (missing before) ---\n",
    "def longest_streak_bool(arr: pd.Series) -> int:\n",
    "    m = 0; cur = 0\n",
    "    for v in arr.astype(bool):\n",
    "        if v: cur += 1; m = max(m, cur)\n",
    "        else: cur = 0\n",
    "    return m\n",
    "\n",
    "# --- daily features ---\n",
    "GDD_BASE = 10.0  # °C\n",
    "HEAT_C   = 35.0  # °C\n",
    "\n",
    "def build_daily(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = raw.copy()\n",
    "    d[\"Year\"]  = d[\"Date\"].dt.year\n",
    "    d[\"Month\"] = d[\"Date\"].dt.month\n",
    "    # SI units\n",
    "    d[\"ppt_mm\"]     = in_to_mm(d[\"ppt (inches)\"])\n",
    "    d[\"tmin_C\"]     = f_to_c(d[\"tmin (degrees F)\"])\n",
    "    d[\"tmean_C\"]    = f_to_c(d[\"tmean (degrees F)\"])\n",
    "    d[\"tmax_C\"]     = f_to_c(d[\"tmax (degrees F)\"])\n",
    "    d[\"tdmean_C\"]   = f_to_c(d[\"tdmean (degrees F)\"])\n",
    "    d[\"vpdmin_kPa\"] = hpa_to_kpa(d[\"vpdmin (hPa)\"])\n",
    "    d[\"vpdmax_kPa\"] = hpa_to_kpa(d[\"vpdmax (hPa)\"])\n",
    "    d[\"vpdmean_kPa\"]= d[[\"vpdmin_kPa\",\"vpdmax_kPa\"]].mean(axis=1)\n",
    "    d[\"Elev_m\"]     = ft_to_m(d[\"Elevation (ft)\"])\n",
    "    # thermal\n",
    "    d[\"GDD_day\"]    = np.maximum(0.0, d[\"tmean_C\"] - GDD_BASE)\n",
    "    d[\"HeatDay\"]    = (d[\"tmax_C\"] > HEAT_C).astype(int)\n",
    "    d[\"FrostDay\"]   = (d[\"tmin_C\"] < 0.0).astype(int)\n",
    "    d[\"ThermalAmp\"] = d[\"tmax_C\"] - d[\"tmin_C\"]\n",
    "    # water balance + spells\n",
    "    d[\"ET0_proxy\"]  = d[\"vpdmean_kPa\"] * (d[\"tmean_C\"] + 5).clip(lower=0)  # ET index\n",
    "    d[\"P_minus_ET\"] = d[\"ppt_mm\"] - d[\"ET0_proxy\"]\n",
    "    d[\"DryDay\"]     = (d[\"ppt_mm\"] < 1.0).astype(int)      # <1 mm\n",
    "    d[\"HeavyRain\"]  = (d[\"ppt_mm\"] > 20.0).astype(int)     # >20 mm\n",
    "    # VPD stress + humidity\n",
    "    d[\"VPD_hi_day\"] = (d[\"vpdmax_kPa\"] > 2.0).astype(int)\n",
    "    d[\"VPD_load\"]   = np.maximum(0.0, d[\"vpdmax_kPa\"] - 2.0)\n",
    "    d[\"DewDep_C\"]   = d[\"tmean_C\"] - d[\"tdmean_C\"]\n",
    "    return d\n",
    "\n",
    "# ---- emergence detection ----\n",
    "def detect_emergence(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for (name, yr), g in d.groupby([\"Name\",\"Year\"]):\n",
    "        g = g.sort_values(\"Date\")\n",
    "        g = g[g[\"Date\"] >= pd.Timestamp(yr,3,1)]\n",
    "        if g.empty:\n",
    "            rows.append({\"Name\":name,\"Year\":yr,\"Emergence\":pd.NaT}); continue\n",
    "        g = g.assign(roll5_Tmean=g[\"tmean_C\"].rolling(5, min_periods=3).mean(),\n",
    "                     cumGDD=g[\"GDD_day\"].cumsum())\n",
    "        hit = g[(g[\"roll5_Tmean\"]>10.0) & (g[\"cumGDD\"]>=50.0)]\n",
    "        emer = hit[\"Date\"].iloc[0] if not hit.empty else pd.NaT\n",
    "        rows.append({\"Name\":name,\"Year\":yr,\"Emergence\":emer})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---- stage tagging by Days After Emergence (DAE) ----\n",
    "CORN_STAGES = [\n",
    "    (\"VE\",   0,   10),\n",
    "    (\"V6\",  25,   30),\n",
    "    (\"VT\",  55,   59),\n",
    "    (\"R1\",  60,   70),\n",
    "    (\"R2\",  71,   80),\n",
    "    (\"R3\",  81,   90),\n",
    "    (\"R4\",  91,  105),\n",
    "    (\"R5\", 106,  125),\n",
    "    (\"R6\", 126,  160),\n",
    "]\n",
    "\n",
    "def tag_corn_stages(d: pd.DataFrame, emer_tbl: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = d.merge(emer_tbl, on=[\"Name\",\"Year\"], how=\"left\")\n",
    "    m[\"DAE\"] = (m[\"Date\"] - m[\"Emergence\"]).dt.days\n",
    "    def stage_from_dae(v):\n",
    "        if pd.isna(v): return None\n",
    "        for lab, lo, hi in CORN_STAGES:\n",
    "            if lo <= v <= hi: return lab\n",
    "        return None\n",
    "    m[\"Stage\"] = m[\"DAE\"].apply(stage_from_dae)\n",
    "    return m\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def corn_stage_aggregate(tagged: pd.DataFrame) -> pd.DataFrame:\n",
    "    # filter and order\n",
    "    g = tagged.dropna(subset=[\"Stage\"]).copy()\n",
    "    g = g.sort_values([\"Name\",\"Year\",\"Stage\",\"Date\"])\n",
    "\n",
    "    # per-stage aggregates\n",
    "    core = (g.groupby([\"Name\",\"Year\",\"Stage\"], as_index=False)\n",
    "              .agg(\n",
    "                  # thermal\n",
    "                  GDD_sum=(\"GDD_day\",\"sum\"),\n",
    "                  HeatDays=(\"HeatDay\",\"sum\"),\n",
    "                  FrostDays=(\"FrostDay\",\"sum\"),\n",
    "                  Tmean_C=(\"tmean_C\",\"mean\"),\n",
    "                  Tmax_C=(\"tmax_C\",\"mean\"),\n",
    "                  ThermalAmp=(\"ThermalAmp\",\"mean\"),\n",
    "                  # water\n",
    "                  PPT_mm=(\"ppt_mm\",\"sum\"),\n",
    "                  ET0_sum=(\"ET0_proxy\",\"sum\"),\n",
    "                  P_minus_ET=(\"P_minus_ET\",\"sum\"),\n",
    "                  DryDays=(\"DryDay\",\"sum\"),\n",
    "                  HeavyRainDays=(\"HeavyRain\",\"sum\"),\n",
    "                  # VPD / humidity\n",
    "                  VPDmax_kPa=(\"vpdmax_kPa\",\"mean\"),\n",
    "                  VPDmean_kPa=(\"vpdmean_kPa\",\"mean\"),\n",
    "                  VPD_hi_days=(\"VPD_hi_day\",\"sum\"),\n",
    "                  VPD_load_sum=(\"VPD_load\",\"sum\"),\n",
    "                  DewDep_C=(\"DewDep_C\",\"mean\"),\n",
    "              ))\n",
    "\n",
    "    # longest streaks without the deprecated .apply-on-groups behavior\n",
    "    grp = g.groupby([\"Name\",\"Year\",\"Stage\"], observed=True)\n",
    "    streaks = (pd.DataFrame({\n",
    "        \"LongestDryStreak\": grp[\"DryDay\"].apply(longest_streak_bool),\n",
    "        \"LongestHotStreak\": grp[\"HeatDay\"].apply(longest_streak_bool),\n",
    "    }).reset_index())\n",
    "\n",
    "    out = core.merge(streaks, on=[\"Name\",\"Year\",\"Stage\"], how=\"left\")\n",
    "\n",
    "    # safe ratio\n",
    "    out[\"EffPrecRatio_idx\"] = out[\"PPT_mm\"] / out[\"ET0_sum\"].replace(0, np.nan)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---- run ----\n",
    "# assumes you already defined/load: load_prism_all, DATA_DIR, PATTERN\n",
    "raw = load_prism_all(DATA_DIR, PATTERN)\n",
    "daily = build_daily(raw)\n",
    "emer  = detect_emergence(daily)\n",
    "corn_tagged = tag_corn_stages(daily, emer)\n",
    "corn_stage  = corn_stage_aggregate(corn_tagged)\n",
    "\n",
    "daily.to_parquet(\"Kashish_results/corn_prism_daily.parquet\", index=False)\n",
    "emer.to_csv(\"Kashish_results/corn_emergence_dates.csv\", index=False)\n",
    "corn_tagged.to_parquet(\"Kashish_results/corn_daily_with_stages.parquet\", index=False)\n",
    "corn_stage.to_csv(\"Kashish_results/corn_stage_features.csv\", index=False)\n",
    "\n",
    "print(\"Rows daily:\", len(daily), \"| years:\", daily['Year'].min(), \"-\", daily['Year'].max())\n",
    "print(\"Stage rows:\", len(corn_stage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd8d7843-46ed-472e-bf2a-25995333eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Wide stage table per Name×Year\n",
    "import pandas as pd, os\n",
    "os.makedirs(\"Kashish_results\", exist_ok=True)\n",
    "\n",
    "corn_stage = pd.read_csv(\"Kashish_results/corn_stage_features.csv\")\n",
    "wide = (corn_stage.pivot_table(\n",
    "    index=[\"Name\",\"Year\"], columns=\"Stage\",\n",
    "    values=[\"PPT_mm\",\"ET0_sum\",\"P_minus_ET\",\"EffPrecRatio_idx\",\n",
    "            \"GDD_sum\",\"HeatDays\",\"FrostDays\",\"LongestDryStreak\",\"LongestHotStreak\",\n",
    "            \"Tmean_C\",\"Tmax_C\",\"ThermalAmp\",\n",
    "            \"VPDmax_kPa\",\"VPDmean_kPa\",\"VPD_hi_days\",\"VPD_load_sum\",\"DewDep_C\"],\n",
    "    aggfunc=\"first\")\n",
    "    .reset_index())\n",
    "wide.columns = [\"_\".join(c).strip(\"_\") for c in wide.columns.to_flat_index()]\n",
    "wide.to_csv(\"Kashish_results/corn_stage_features_wide_plus.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30b13fec-57d1-4709-87c3-ba6abdad00f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18105 entries, 0 to 18104\n",
      "Data columns (total 23 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          18105 non-null  int64  \n",
      " 1   Program             18105 non-null  object \n",
      " 2   Year                18105 non-null  int64  \n",
      " 3   Period              18105 non-null  object \n",
      " 4   Week Ending         0 non-null      float64\n",
      " 5   Geo Level           18105 non-null  object \n",
      " 6   State               18105 non-null  object \n",
      " 7   State ANSI          18105 non-null  int64  \n",
      " 8   Ag District         18089 non-null  object \n",
      " 9   Ag District Code    18105 non-null  int64  \n",
      " 10  County              18105 non-null  object \n",
      " 11  County ANSI         17359 non-null  float64\n",
      " 12  Zip Code            0 non-null      float64\n",
      " 13  Region              0 non-null      float64\n",
      " 14  watershed_code      18105 non-null  int64  \n",
      " 15  Watershed           0 non-null      float64\n",
      " 16  Commodity           18105 non-null  object \n",
      " 17  Data Item           18105 non-null  object \n",
      " 18  Domain              18105 non-null  object \n",
      " 19  Domain Category     18105 non-null  object \n",
      " 20  Value               18105 non-null  float64\n",
      " 21  CV (%)              783 non-null    float64\n",
      " 22  YIELD_LBS_PER_ACRE  18105 non-null  float64\n",
      "dtypes: float64(8), int64(5), object(10)\n",
      "memory usage: 3.2+ MB\n",
      "None\n",
      "   Unnamed: 0 Program  Year Period  Week Ending Geo Level           State  \\\n",
      "0         403  SURVEY  2022   YEAR          NaN    COUNTY  NORTH CAROLINA   \n",
      "1         404  SURVEY  2022   YEAR          NaN    COUNTY  NORTH CAROLINA   \n",
      "2         405  SURVEY  2022   YEAR          NaN    COUNTY  NORTH CAROLINA   \n",
      "3         406  SURVEY  2022   YEAR          NaN    COUNTY  NORTH CAROLINA   \n",
      "4         407  SURVEY  2022   YEAR          NaN    COUNTY  NORTH CAROLINA   \n",
      "\n",
      "   State ANSI Ag District  Ag District Code  ... Region  watershed_code  \\\n",
      "0          37         NaN                99  ...    NaN               0   \n",
      "1          37         NaN                99  ...    NaN               0   \n",
      "2          37         NaN                99  ...    NaN               0   \n",
      "3          37         NaN                99  ...    NaN               0   \n",
      "4          37         NaN                99  ...    NaN               0   \n",
      "\n",
      "   Watershed  Commodity                                      Data Item  \\\n",
      "0        NaN       CORN     CORN, GRAIN - YIELD, MEASURED IN BU / ACRE   \n",
      "1        NaN       CORN  CORN, SILAGE - YIELD, MEASURED IN TONS / ACRE   \n",
      "2        NaN     COTTON  COTTON, UPLAND - YIELD, MEASURED IN LB / ACRE   \n",
      "3        NaN    PEANUTS         PEANUTS - YIELD, MEASURED IN LB / ACRE   \n",
      "4        NaN   SOYBEANS        SOYBEANS - YIELD, MEASURED IN BU / ACRE   \n",
      "\n",
      "   Domain Domain Category   Value CV (%) YIELD_LBS_PER_ACRE  \n",
      "0   TOTAL   NOT SPECIFIED   122.2    3.2             6843.2  \n",
      "1   TOTAL   NOT SPECIFIED    14.0    NaN            28000.0  \n",
      "2   TOTAL   NOT SPECIFIED   871.0    3.0              871.0  \n",
      "3   TOTAL   NOT SPECIFIED  4065.0    1.9             4065.0  \n",
      "4   TOTAL   NOT SPECIFIED    36.9    1.9             2214.0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "yld_full = pd.read_csv(\"Daily_for_transform/crop_yield_1980-2022.csv\")\n",
    "print(yld_full.info())\n",
    "print(yld_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12f38beb-3aa0-4ca5-bfaa-2d8f1dbf520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year            Name Commodity    Yield\n",
      "0  2022  OTHER COUNTIES      CORN   6843.2\n",
      "1  2022  OTHER COUNTIES      CORN  28000.0\n",
      "2  2022  OTHER COUNTIES    COTTON    871.0\n",
      "3  2022  OTHER COUNTIES   PEANUTS   4065.0\n",
      "4  2022  OTHER COUNTIES  SOYBEANS   2214.0\n",
      "['CORN' 'COTTON' 'PEANUTS' 'SOYBEANS' 'WHEAT']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "yld = pd.read_csv(\"crop_yield_1980-2022.csv\")\n",
    "\n",
    "# keep only necessary columns\n",
    "yld_clean = yld[[\"Year\",\"County\",\"Commodity\",\"YIELD_LBS_PER_ACRE\"]].copy()\n",
    "yld_clean = yld_clean.rename(columns={\"County\":\"Name\", \"YIELD_LBS_PER_ACRE\":\"Yield\"})\n",
    "\n",
    "# check\n",
    "print(yld_clean.head())\n",
    "print(yld_clean[\"Commodity\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a1439ae-4601-4157-a16d-16bf52e2656f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 3327 years: 1983 - 2022\n",
      "cols: 157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "      <th>Commodity</th>\n",
       "      <th>Yield</th>\n",
       "      <th>DewDep_C_R1</th>\n",
       "      <th>DewDep_C_R2</th>\n",
       "      <th>DewDep_C_R3</th>\n",
       "      <th>DewDep_C_R4</th>\n",
       "      <th>DewDep_C_R5</th>\n",
       "      <th>DewDep_C_R6</th>\n",
       "      <th>...</th>\n",
       "      <th>VPDmax_kPa_VT</th>\n",
       "      <th>VPDmean_kPa_R1</th>\n",
       "      <th>VPDmean_kPa_R2</th>\n",
       "      <th>VPDmean_kPa_R3</th>\n",
       "      <th>VPDmean_kPa_R4</th>\n",
       "      <th>VPDmean_kPa_R5</th>\n",
       "      <th>VPDmean_kPa_R6</th>\n",
       "      <th>VPDmean_kPa_V6</th>\n",
       "      <th>VPDmean_kPa_VE</th>\n",
       "      <th>VPDmean_kPa_VT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>BEAUFORT</td>\n",
       "      <td>CORN</td>\n",
       "      <td>8064.0</td>\n",
       "      <td>5.318182</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>6.977778</td>\n",
       "      <td>6.529630</td>\n",
       "      <td>4.252778</td>\n",
       "      <td>4.544444</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9876</td>\n",
       "      <td>0.802091</td>\n",
       "      <td>0.98625</td>\n",
       "      <td>1.49950</td>\n",
       "      <td>1.294133</td>\n",
       "      <td>1.019650</td>\n",
       "      <td>1.125386</td>\n",
       "      <td>0.923417</td>\n",
       "      <td>0.532318</td>\n",
       "      <td>1.0399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>CRAVEN</td>\n",
       "      <td>CORN</td>\n",
       "      <td>6501.6</td>\n",
       "      <td>5.323232</td>\n",
       "      <td>5.150000</td>\n",
       "      <td>7.227778</td>\n",
       "      <td>6.796296</td>\n",
       "      <td>4.580556</td>\n",
       "      <td>4.726984</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1604</td>\n",
       "      <td>0.796773</td>\n",
       "      <td>1.06315</td>\n",
       "      <td>1.49965</td>\n",
       "      <td>1.326567</td>\n",
       "      <td>1.080100</td>\n",
       "      <td>1.148000</td>\n",
       "      <td>1.003167</td>\n",
       "      <td>0.551182</td>\n",
       "      <td>1.1756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>GREENE</td>\n",
       "      <td>CORN</td>\n",
       "      <td>5555.2</td>\n",
       "      <td>5.858586</td>\n",
       "      <td>5.055556</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>7.548148</td>\n",
       "      <td>5.144444</td>\n",
       "      <td>4.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0600</td>\n",
       "      <td>0.883409</td>\n",
       "      <td>1.11840</td>\n",
       "      <td>1.66560</td>\n",
       "      <td>1.495800</td>\n",
       "      <td>1.273425</td>\n",
       "      <td>1.196214</td>\n",
       "      <td>1.035750</td>\n",
       "      <td>0.657545</td>\n",
       "      <td>1.0583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>HYDE</td>\n",
       "      <td>CORN</td>\n",
       "      <td>8797.6</td>\n",
       "      <td>4.045455</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>6.061111</td>\n",
       "      <td>6.103704</td>\n",
       "      <td>4.144444</td>\n",
       "      <td>4.726984</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5802</td>\n",
       "      <td>0.558545</td>\n",
       "      <td>0.82400</td>\n",
       "      <td>1.18555</td>\n",
       "      <td>1.131633</td>\n",
       "      <td>0.901525</td>\n",
       "      <td>1.098243</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.473273</td>\n",
       "      <td>0.8330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>JOHNSTON</td>\n",
       "      <td>CORN</td>\n",
       "      <td>5448.8</td>\n",
       "      <td>5.464646</td>\n",
       "      <td>7.038889</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>7.659259</td>\n",
       "      <td>4.694444</td>\n",
       "      <td>5.084127</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4062</td>\n",
       "      <td>1.123455</td>\n",
       "      <td>1.43585</td>\n",
       "      <td>1.38205</td>\n",
       "      <td>1.447267</td>\n",
       "      <td>1.142650</td>\n",
       "      <td>1.191943</td>\n",
       "      <td>1.227167</td>\n",
       "      <td>0.853318</td>\n",
       "      <td>0.7384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year      Name Commodity   Yield  DewDep_C_R1  DewDep_C_R2  DewDep_C_R3  \\\n",
       "0  2022  BEAUFORT      CORN  8064.0     5.318182     4.583333     6.977778   \n",
       "1  2022    CRAVEN      CORN  6501.6     5.323232     5.150000     7.227778   \n",
       "2  2022    GREENE      CORN  5555.2     5.858586     5.055556     8.050000   \n",
       "3  2022      HYDE      CORN  8797.6     4.045455     4.222222     6.061111   \n",
       "4  2022  JOHNSTON      CORN  5448.8     5.464646     7.038889     6.833333   \n",
       "\n",
       "   DewDep_C_R4  DewDep_C_R5  DewDep_C_R6  ...  VPDmax_kPa_VT  VPDmean_kPa_R1  \\\n",
       "0     6.529630     4.252778     4.544444  ...         1.9876        0.802091   \n",
       "1     6.796296     4.580556     4.726984  ...         2.1604        0.796773   \n",
       "2     7.548148     5.144444     4.777778  ...         2.0600        0.883409   \n",
       "3     6.103704     4.144444     4.726984  ...         1.5802        0.558545   \n",
       "4     7.659259     4.694444     5.084127  ...         1.4062        1.123455   \n",
       "\n",
       "   VPDmean_kPa_R2  VPDmean_kPa_R3  VPDmean_kPa_R4  VPDmean_kPa_R5  \\\n",
       "0         0.98625         1.49950        1.294133        1.019650   \n",
       "1         1.06315         1.49965        1.326567        1.080100   \n",
       "2         1.11840         1.66560        1.495800        1.273425   \n",
       "3         0.82400         1.18555        1.131633        0.901525   \n",
       "4         1.43585         1.38205        1.447267        1.142650   \n",
       "\n",
       "   VPDmean_kPa_R6  VPDmean_kPa_V6  VPDmean_kPa_VE  VPDmean_kPa_VT  \n",
       "0        1.125386        0.923417        0.532318          1.0399  \n",
       "1        1.148000        1.003167        0.551182          1.1756  \n",
       "2        1.196214        1.035750        0.657545          1.0583  \n",
       "3        1.098243        0.744500        0.473273          0.8330  \n",
       "4        1.191943        1.227167        0.853318          0.7384  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# inputs\n",
    "yld_path  = \"crop_yield_1980-2022.csv\"\n",
    "wide_path = \"Kashish_results/corn_stage_features_wide_plus.csv\"  # or ..._wide.csv\n",
    "\n",
    "# 1) clean yields\n",
    "yld = pd.read_csv(yld_path)\n",
    "yld_corn = (yld.loc[yld[\"Commodity\"].str.upper()==\"CORN\", [\"Year\",\"County\",\"Commodity\",\"YIELD_LBS_PER_ACRE\"]]\n",
    "               .rename(columns={\"County\":\"Name\",\"YIELD_LBS_PER_ACRE\":\"Yield\"}))\n",
    "\n",
    "# 2) load stage-wide features\n",
    "wide = pd.read_csv(wide_path)\n",
    "\n",
    "# 3) basic sanity\n",
    "assert {\"Name\",\"Year\"}.issubset(wide.columns), \"Wide table missing Name/Year.\"\n",
    "\n",
    "# 4) merge\n",
    "corn_df = (yld_corn.merge(wide, on=[\"Name\",\"Year\"], how=\"inner\")\n",
    "                    .dropna()\n",
    "                    .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "\n",
    "# 5) save\n",
    "Path(\"Kashish_results\").mkdir(exist_ok=True, parents=True)\n",
    "out_path = \"Kashish_results/corn_model_table.csv\"\n",
    "corn_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"rows:\", len(corn_df), \"years:\", corn_df['Year'].min(), \"-\", corn_df['Year'].max())\n",
    "print(\"cols:\", len(corn_df.columns))\n",
    "corn_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2221e00b-acb8-4fb3-9151-b531ce505cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soybean stage rows: 29400 | wide rows: (4200, 121)\n"
     ]
    }
   ],
   "source": [
    "# soybean_pipeline.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- config ---\n",
    "DATA_DIR = Path(\"Daily_for_transform/Daily Climate Data\")\n",
    "PATTERN  = \"PRISM_ppt_tmin_tmean_tmax_tdmean_vpdmin_vpdmax_stable_800m_*.csv\"\n",
    "OUTDIR   = Path(\"Kashish_results\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- soybean phenology (DAE windows) ----------\n",
    "SOYBEAN_STAGES = [\n",
    "    (\"VE\",    0,   7),\n",
    "    (\"V2_V3\", 8,  25),\n",
    "    (\"V4_V6\", 26, 45),\n",
    "    (\"R1_R2\", 46, 65),   # flowering\n",
    "    (\"R3_R4\", 66, 85),   # pod set\n",
    "    (\"R5_R6\", 86, 115),  # seed fill\n",
    "    (\"R7_R8\", 116, 140), # maturity\n",
    "]\n",
    "\n",
    "def tag_soybean_stages(d: pd.DataFrame, emer_tbl: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = d.merge(emer_tbl, on=[\"Name\",\"Year\"], how=\"left\")\n",
    "    m[\"DAE\"] = (m[\"Date\"] - m[\"Emergence\"]).dt.days\n",
    "    def stage_from_dae(v):\n",
    "        if pd.isna(v): return None\n",
    "        for lab, lo, hi in SOYBEAN_STAGES:\n",
    "            if lo <= v <= hi: return lab\n",
    "        return None\n",
    "    m[\"Stage\"] = m[\"DAE\"].apply(stage_from_dae)\n",
    "    return m\n",
    "\n",
    "def soybean_stage_aggregate(tagged: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = tagged.dropna(subset=[\"Stage\"]).copy()\n",
    "    g = g.sort_values([\"Name\",\"Year\",\"Stage\",\"Date\"])\n",
    "    core = (g.groupby([\"Name\",\"Year\",\"Stage\"], as_index=False)\n",
    "              .agg(\n",
    "                  # thermal\n",
    "                  GDD_sum=(\"GDD_day\",\"sum\"),\n",
    "                  HeatDays=(\"HeatDay\",\"sum\"),\n",
    "                  FrostDays=(\"FrostDay\",\"sum\"),\n",
    "                  Tmean_C=(\"tmean_C\",\"mean\"),\n",
    "                  Tmax_C=(\"tmax_C\",\"mean\"),\n",
    "                  ThermalAmp=(\"ThermalAmp\",\"mean\"),\n",
    "                  # water\n",
    "                  PPT_mm=(\"ppt_mm\",\"sum\"),\n",
    "                  ET0_sum=(\"ET0_proxy\",\"sum\"),\n",
    "                  P_minus_ET=(\"P_minus_ET\",\"sum\"),\n",
    "                  DryDays=(\"DryDay\",\"sum\"),\n",
    "                  HeavyRainDays=(\"HeavyRain\",\"sum\"),\n",
    "                  # VPD / humidity\n",
    "                  VPDmax_kPa=(\"vpdmax_kPa\",\"mean\"),\n",
    "                  VPDmean_kPa=(\"vpdmean_kPa\",\"mean\"),\n",
    "                  VPD_hi_days=(\"VPD_hi_day\",\"sum\"),\n",
    "                  VPD_load_sum=(\"VPD_load\",\"sum\"),\n",
    "                  DewDep_C=(\"DewDep_C\",\"mean\"),\n",
    "              ))\n",
    "    # longest streaks (uses your longest_streak_bool)\n",
    "    grp = g.groupby([\"Name\",\"Year\",\"Stage\"], observed=True)\n",
    "    streaks = (pd.DataFrame({\n",
    "        \"LongestDryStreak\": grp[\"DryDay\"].apply(longest_streak_bool),\n",
    "        \"LongestHotStreak\": grp[\"HeatDay\"].apply(longest_streak_bool),\n",
    "    }).reset_index())\n",
    "    out = core.merge(streaks, on=[\"Name\",\"Year\",\"Stage\"], how=\"left\")\n",
    "    out[\"EffPrecRatio_idx\"] = out[\"PPT_mm\"] / out[\"ET0_sum\"].replace(0, np.nan)\n",
    "    return out\n",
    "\n",
    "# ---------- run ----------\n",
    "# reuse your robust loader and daily builder\n",
    "raw   = load_prism_all(DATA_DIR, PATTERN)   # already tested for corn\n",
    "daily = build_daily(raw)                    # already extended with ET/VPD features\n",
    "emer  = detect_emergence(daily)             # VE per Name×Year\n",
    "\n",
    "# stage tagging and aggregation\n",
    "soy_tagged = tag_soybean_stages(daily, emer)\n",
    "soy_stage  = soybean_stage_aggregate(soy_tagged)\n",
    "\n",
    "# save long and wide\n",
    "soy_tagged.to_parquet(OUTDIR/\"soybean_daily_with_stages.parquet\", index=False)\n",
    "soy_stage.to_csv(OUTDIR/\"soybean_stage_features.csv\", index=False)\n",
    "\n",
    "wide = (soy_stage.pivot_table(\n",
    "    index=[\"Name\",\"Year\"], columns=\"Stage\",\n",
    "    values=[\"PPT_mm\",\"ET0_sum\",\"P_minus_ET\",\"EffPrecRatio_idx\",\n",
    "            \"GDD_sum\",\"HeatDays\",\"FrostDays\",\"LongestDryStreak\",\"LongestHotStreak\",\n",
    "            \"Tmean_C\",\"Tmax_C\",\"ThermalAmp\",\n",
    "            \"VPDmax_kPa\",\"VPDmean_kPa\",\"VPD_hi_days\",\"VPD_load_sum\",\"DewDep_C\"],\n",
    "    aggfunc=\"first\").reset_index())\n",
    "wide.columns = [\"_\".join(c).strip(\"_\") for c in wide.columns.to_flat_index()]\n",
    "wide.to_csv(OUTDIR/\"soybean_stage_features_wide_plus.csv\", index=False)\n",
    "\n",
    "print(\"Soybean stage rows:\", len(soy_stage), \"| wide rows:\", wide.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5bc4df3d-d5e6-41da-b827-dca878f30893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model table: (3096, 123) | 2023 inference: (100, 121)\n"
     ]
    }
   ],
   "source": [
    "# merge soybean yields\n",
    "yld = pd.read_csv(\"crop_yield_1980-2022.csv\")\n",
    "yld_soy = (yld.loc[yld[\"Commodity\"].str.upper()==\"SOYBEANS\", [\"Year\",\"County\",\"Commodity\",\"YIELD_LBS_PER_ACRE\"]]\n",
    "              .rename(columns={\"County\":\"Name\",\"YIELD_LBS_PER_ACRE\":\"Yield\"}))\n",
    "\n",
    "soy_df = (yld_soy.merge(wide[wide[\"Year\"]<=2022], on=[\"Name\",\"Year\"], how=\"inner\")\n",
    "                 .dropna()\n",
    "                 .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "soy_df.to_csv(OUTDIR/\"soybean_model_table.csv\", index=False)\n",
    "\n",
    "# inference table for 2023\n",
    "soy_infer_2023 = (wide[wide[\"Year\"]==2023]\n",
    "                  .dropna()\n",
    "                  .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "soy_infer_2023.to_csv(OUTDIR/\"soybean_inference_2023.csv\", index=False)\n",
    "\n",
    "print(\"Model table:\", soy_df.shape, \"| 2023 inference:\", soy_infer_2023.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "591dbab9-7089-4f0e-9be9-90a076f6d8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peanut stage rows: 25200 | wide rows: (4200, 104)\n",
      "Model table: (938, 106) | 2023 inference: (100, 104)\n",
      "Saved: Kashish_results/peanuts/peanut_predictions_2023.csv\n"
     ]
    }
   ],
   "source": [
    "# peanut_pipeline.py\n",
    "import pandas as pd, numpy as np, re, os\n",
    "from pathlib import Path\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "DATA_DIR = Path(\"Daily_for_transform/Daily Climate Data\")\n",
    "PATTERN  = \"PRISM_ppt_tmin_tmean_tmax_tdmean_vpdmin_vpdmax_stable_800m_*.csv\"\n",
    "OUTDIR   = Path(\"Kashish_results/peanuts\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "YIELD_CSV = \"crop_yield_1980-2022.csv\"\n",
    "\n",
    "# =================== LOADER (robust) ===================\n",
    "EXP = [\"Name\",\"Longitude\",\"Latitude\",\"Elevation (ft)\",\"Date\",\n",
    "       \"ppt (inches)\",\"tmin (degrees F)\",\"tmean (degrees F)\",\"tmax (degrees F)\",\n",
    "       \"tdmean (degrees F)\",\"vpdmin (hPa)\",\"vpdmax (hPa)\"]\n",
    "\n",
    "def _clean_num(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = re.sub(r'(?<=\\d)\\.(?=\\s*$)', '', str(s).strip())\n",
    "    s = re.sub(r'[^0-9.\\-eE]', '', s)\n",
    "    try: return float(s)\n",
    "    except: return np.nan\n",
    "\n",
    "def _find_header_idx(path: Path, max_scan=80):\n",
    "    with path.open(\"r\", encoding=\"utf-8-sig\", errors=\"ignore\") as f:\n",
    "        for i in range(max_scan):\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            low = line.strip().lower()\n",
    "            if low.startswith(\"name,longitude,latitude,elevation (ft),date,ppt\"): return i\n",
    "            if \"name,\" in low and \",date,\" in low: return i\n",
    "    return None\n",
    "\n",
    "def read_one_prism(path: Path) -> pd.DataFrame:\n",
    "    hdr = _find_header_idx(path)\n",
    "    if hdr is None: hdr = 9\n",
    "    df = pd.read_csv(path, header=hdr, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    if \"Date\" in df.columns:\n",
    "        df = df[df[\"Date\"].astype(str).str.match(r\"\\d{4}-\\d{2}-\\d{2}\", na=False)]\n",
    "    miss = [c for c in EXP if c not in df.columns]\n",
    "    if miss: raise ValueError(f\"{path.name}: missing {miss}\")\n",
    "    for c in EXP:\n",
    "        if c not in [\"Name\",\"Date\"]:\n",
    "            df[c] = df[c].apply(_clean_num)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Date\"]).copy()\n",
    "    df[\"__src\"] = path.name\n",
    "    return df\n",
    "\n",
    "def load_prism_all(root: Path, pattern: str) -> pd.DataFrame:\n",
    "    files = sorted(root.rglob(pattern))\n",
    "    if not files: raise FileNotFoundError(f\"No files under {root.resolve()}\")\n",
    "    parts, errs = [], []\n",
    "    for p in files:\n",
    "        try: parts.append(read_one_prism(p))\n",
    "        except Exception as e: errs.append((p.name, str(e)))\n",
    "    if not parts: raise RuntimeError(\"Parsed 0 valid tables.\")\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "    out = out.drop_duplicates(subset=[\"Name\",\"Date\",\"ppt (inches)\",\"tmin (degrees F)\",\n",
    "                                      \"tmean (degrees F)\",\"tmax (degrees F)\",\"tdmean (degrees F)\",\n",
    "                                      \"vpdmin (hPa)\",\"vpdmax (hPa)\"])\n",
    "    return out\n",
    "\n",
    "# =================== DAILY FEATURES ===================\n",
    "f_to_c     = lambda f: (f-32.0)*(5/9)\n",
    "in_to_mm   = lambda x: x*25.4\n",
    "ft_to_m    = lambda x: x*0.3048\n",
    "hpa_to_kpa = lambda x: x*0.1\n",
    "\n",
    "HEAT_C   = 35.0  # generic heat-stress threshold\n",
    "\n",
    "def build_daily(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = raw.copy()\n",
    "    d[\"Year\"]  = d[\"Date\"].dt.year\n",
    "    d[\"Month\"] = d[\"Date\"].dt.month\n",
    "    # SI and derivatives\n",
    "    d[\"ppt_mm\"]     = in_to_mm(d[\"ppt (inches)\"])\n",
    "    d[\"tmin_C\"]     = f_to_c(d[\"tmin (degrees F)\"])\n",
    "    d[\"tmean_C\"]    = f_to_c(d[\"tmean (degrees F)\"])\n",
    "    d[\"tmax_C\"]     = f_to_c(d[\"tmax (degrees F)\"])\n",
    "    d[\"tdmean_C\"]   = f_to_c(d[\"tdmean (degrees F)\"])\n",
    "    d[\"vpdmin_kPa\"] = hpa_to_kpa(d[\"vpdmin (hPa)\"])\n",
    "    d[\"vpdmax_kPa\"] = hpa_to_kpa(d[\"vpdmax (hPa)\"])\n",
    "    d[\"vpdmean_kPa\"]= d[[\"vpdmin_kPa\",\"vpdmax_kPa\"]].mean(axis=1)\n",
    "    d[\"Elev_m\"]     = ft_to_m(d[\"Elevation (ft)\"])\n",
    "    # generic thermal\n",
    "    d[\"HeatDay\"]    = (d[\"tmax_C\"] > HEAT_C).astype(int)\n",
    "    d[\"FrostDay\"]   = (d[\"tmin_C\"] < 0.0).astype(int)\n",
    "    d[\"ThermalAmp\"] = d[\"tmax_C\"] - d[\"tmin_C\"]\n",
    "    # water + VPD\n",
    "    d[\"ET0_proxy\"]  = d[\"vpdmean_kPa\"] * (d[\"tmean_C\"] + 5).clip(lower=0)\n",
    "    d[\"P_minus_ET\"] = d[\"ppt_mm\"] - d[\"ET0_proxy\"]\n",
    "    d[\"DryDay\"]     = (d[\"ppt_mm\"] < 1.0).astype(int)\n",
    "    d[\"HeavyRain\"]  = (d[\"ppt_mm\"] > 20.0).astype(int)\n",
    "    d[\"VPD_hi_day\"] = (d[\"vpdmax_kPa\"] > 2.0).astype(int)\n",
    "    d[\"VPD_load\"]   = np.maximum(0.0, d[\"vpdmax_kPa\"] - 2.0)\n",
    "    d[\"DewDep_C\"]   = d[\"tmean_C\"] - d[\"tdmean_C\"]\n",
    "    return d\n",
    "\n",
    "# =================== EMERGENCE (PEANUT-SPECIFIC) ===================\n",
    "# Use base 12.8 °C for detection.\n",
    "def detect_emergence_peanut(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for (name, yr), g in d.groupby([\"Name\",\"Year\"]):\n",
    "        g = g.sort_values(\"Date\")\n",
    "        g = g[g[\"Date\"] >= pd.Timestamp(yr,3,1)]\n",
    "        if g.empty: rows.append({\"Name\":name,\"Year\":yr,\"Emergence\":pd.NaT}); continue\n",
    "        g[\"GDD12\"] = np.maximum(0.0, g[\"tmean_C\"] - 12.8)\n",
    "        g[\"roll5_Tmean\"] = g[\"tmean_C\"].rolling(5, min_periods=3).mean()\n",
    "        g[\"cumGDD12\"]    = g[\"GDD12\"].cumsum()\n",
    "        hit = g[(g[\"roll5_Tmean\"]>12.0) & (g[\"cumGDD12\"]>=60.0)]\n",
    "        emer = hit[\"Date\"].iloc[0] if not hit.empty else pd.NaT\n",
    "        rows.append({\"Name\":name,\"Year\":yr,\"Emergence\":emer})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =================== STAGES (PEANUT) ===================\n",
    "PEANUT_STAGES = [\n",
    "    (\"VE\",        0,   10),\n",
    "    (\"VEG\",      11,   30),\n",
    "    (\"PEG\",      31,   60),\n",
    "    (\"POD_SET\",  61,   90),\n",
    "    (\"SEED_FILL\",91,  120),\n",
    "    (\"MATURITY\",121,  150),\n",
    "]\n",
    "\n",
    "def tag_peanut_stages(d: pd.DataFrame, emer_tbl: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = d.merge(emer_tbl, on=[\"Name\",\"Year\"], how=\"left\")\n",
    "    m[\"DAE\"] = (m[\"Date\"] - m[\"Emergence\"]).dt.days\n",
    "    # peanut-specific GDD base 12.8 °C for aggregation\n",
    "    m[\"GDD_day\"] = np.maximum(0.0, m[\"tmean_C\"] - 12.8)\n",
    "    def stage_from_dae(v):\n",
    "        if pd.isna(v): return None\n",
    "        for lab, lo, hi in PEANUT_STAGES:\n",
    "            if lo <= v <= hi: return lab\n",
    "        return None\n",
    "    m[\"Stage\"] = m[\"DAE\"].apply(stage_from_dae)\n",
    "    return m\n",
    "\n",
    "# =================== STREAKS + AGG ===================\n",
    "def longest_streak_bool(arr: pd.Series) -> int:\n",
    "    m = 0; cur = 0\n",
    "    for v in arr.astype(bool):\n",
    "        if v: cur += 1; m = max(m, cur)\n",
    "        else: cur = 0\n",
    "    return m\n",
    "\n",
    "def peanut_stage_aggregate(tagged: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = tagged.dropna(subset=[\"Stage\"]).copy().sort_values([\"Name\",\"Year\",\"Stage\",\"Date\"])\n",
    "    core = (g.groupby([\"Name\",\"Year\",\"Stage\"], as_index=False)\n",
    "              .agg(\n",
    "                  GDD_sum=(\"GDD_day\",\"sum\"),\n",
    "                  HeatDays=(\"HeatDay\",\"sum\"),\n",
    "                  FrostDays=(\"FrostDay\",\"sum\"),\n",
    "                  Tmean_C=(\"tmean_C\",\"mean\"),\n",
    "                  Tmax_C=(\"tmax_C\",\"mean\"),\n",
    "                  ThermalAmp=(\"ThermalAmp\",\"mean\"),\n",
    "                  PPT_mm=(\"ppt_mm\",\"sum\"),\n",
    "                  ET0_sum=(\"ET0_proxy\",\"sum\"),\n",
    "                  P_minus_ET=(\"P_minus_ET\",\"sum\"),\n",
    "                  DryDays=(\"DryDay\",\"sum\"),\n",
    "                  HeavyRainDays=(\"HeavyRain\",\"sum\"),\n",
    "                  VPDmax_kPa=(\"vpdmax_kPa\",\"mean\"),\n",
    "                  VPDmean_kPa=(\"vpdmean_kPa\",\"mean\"),\n",
    "                  VPD_hi_days=(\"VPD_hi_day\",\"sum\"),\n",
    "                  VPD_load_sum=(\"VPD_load\",\"sum\"),\n",
    "                  DewDep_C=(\"DewDep_C\",\"mean\"),\n",
    "              ))\n",
    "    grp = g.groupby([\"Name\",\"Year\",\"Stage\"], observed=True)\n",
    "    streaks = (pd.DataFrame({\n",
    "        \"LongestDryStreak\": grp[\"DryDay\"].apply(longest_streak_bool),\n",
    "        \"LongestHotStreak\": grp[\"HeatDay\"].apply(longest_streak_bool),\n",
    "    }).reset_index())\n",
    "    out = core.merge(streaks, on=[\"Name\",\"Year\",\"Stage\"], how=\"left\")\n",
    "    out[\"EffPrecRatio_idx\"] = out[\"PPT_mm\"] / out[\"ET0_sum\"].replace(0, np.nan)\n",
    "    return out\n",
    "\n",
    "# =================== RUN ===================\n",
    "raw   = load_prism_all(DATA_DIR, PATTERN)\n",
    "daily = build_daily(raw)\n",
    "emer  = detect_emergence_peanut(daily)\n",
    "pea_tagged = tag_peanut_stages(daily, emer)\n",
    "pea_stage  = peanut_stage_aggregate(pea_tagged)\n",
    "\n",
    "# save\n",
    "pea_tagged.to_parquet(OUTDIR/\"peanut_daily_with_stages.parquet\", index=False)\n",
    "pea_stage.to_csv(OUTDIR/\"peanut_stage_features.csv\", index=False)\n",
    "\n",
    "# wide\n",
    "wide = (pea_stage.pivot_table(\n",
    "    index=[\"Name\",\"Year\"], columns=\"Stage\",\n",
    "    values=[\"PPT_mm\",\"ET0_sum\",\"P_minus_ET\",\"EffPrecRatio_idx\",\n",
    "            \"GDD_sum\",\"HeatDays\",\"FrostDays\",\"LongestDryStreak\",\"LongestHotStreak\",\n",
    "            \"Tmean_C\",\"Tmax_C\",\"ThermalAmp\",\n",
    "            \"VPDmax_kPa\",\"VPDmean_kPa\",\"VPD_hi_days\",\"VPD_load_sum\",\"DewDep_C\"],\n",
    "    aggfunc=\"first\").reset_index())\n",
    "wide.columns = [\"_\".join(c).strip(\"_\") for c in wide.columns.to_flat_index()]\n",
    "wide.to_csv(OUTDIR/\"peanut_stage_features_wide_plus.csv\", index=False)\n",
    "\n",
    "print(\"Peanut stage rows:\", len(pea_stage), \"| wide rows:\", wide.shape)\n",
    "\n",
    "# =================== MERGE YIELDS ≤2022 + INFERENCE 2023 ===================\n",
    "yld = pd.read_csv(YIELD_CSV)\n",
    "yld_pea = (yld.loc[yld[\"Commodity\"].str.upper()==\"PEANUTS\", [\"Year\",\"County\",\"Commodity\",\"YIELD_LBS_PER_ACRE\"]]\n",
    "             .rename(columns={\"County\":\"Name\",\"YIELD_LBS_PER_ACRE\":\"Yield\"}))\n",
    "\n",
    "pea_df = (yld_pea.merge(wide[wide[\"Year\"]<=2022], on=[\"Name\",\"Year\"], how=\"inner\")\n",
    "                 .dropna()\n",
    "                 .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "pea_df.to_csv(OUTDIR/\"peanut_model_table.csv\", index=False)\n",
    "\n",
    "pea_infer_2023 = (wide[wide[\"Year\"]==2023]\n",
    "                  .dropna()\n",
    "                  .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "pea_infer_2023.to_csv(OUTDIR/\"peanut_inference_2023.csv\", index=False)\n",
    "\n",
    "print(\"Model table:\", pea_df.shape, \"| 2023 inference:\", pea_infer_2023.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdca21b-27fb-40d5-8997-fb1a52b5e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== BASELINE MODEL → 2023 PREDICTIONS ===================\n",
    "FEATS = [c for c in pea_df.columns if c not in [\"Name\",\"Year\",\"Commodity\",\"Yield\"]]\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, random_state=42\n",
    "    )\n",
    "except Exception:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    model = RandomForestRegressor(n_estimators=500, max_depth=None, random_state=42, n_jobs=-1)\n",
    "\n",
    "model.fit(pea_df[FEATS], pea_df[\"Yield\"])\n",
    "pred23 = model.predict(pea_infer_2023[FEATS])\n",
    "\n",
    "predictions_peanut = pd.DataFrame({\n",
    "    \"Year\": 2023,\n",
    "    \"Commodity\": \"PEANUTS\",\n",
    "    \"Name\": pea_infer_2023[\"Name\"].values,\n",
    "    \"Value\": pred23\n",
    "})\n",
    "predictions_peanut.to_csv(OUTDIR/\"peanut_predictions_2023.csv\", index=False)\n",
    "print(\"Saved:\", OUTDIR/\"peanut_predictions_2023.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8f572b7-a3f4-4597-a4e7-76cc62da2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== BOOTSTRAP HELPERS (run once if not already defined) ====\n",
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path\n",
    "\n",
    "# --- robust PRISM loader (header auto-detect) ---\n",
    "EXP = [\"Name\",\"Longitude\",\"Latitude\",\"Elevation (ft)\",\"Date\",\n",
    "       \"ppt (inches)\",\"tmin (degrees F)\",\"tmean (degrees F)\",\"tmax (degrees F)\",\n",
    "       \"tdmean (degrees F)\",\"vpdmin (hPa)\",\"vpdmax (hPa)\"]\n",
    "\n",
    "def _clean_num(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = re.sub(r'(?<=\\d)\\.(?=\\s*$)', '', str(s).strip())\n",
    "    s = re.sub(r'[^0-9.\\-eE]', '', s)\n",
    "    try: return float(s)\n",
    "    except: return np.nan\n",
    "\n",
    "def _find_header_idx(path: Path, max_scan=80):\n",
    "    with path.open(\"r\", encoding=\"utf-8-sig\", errors=\"ignore\") as f:\n",
    "        for i in range(max_scan):\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            low = line.strip().lower()\n",
    "            if low.startswith(\"name,longitude,latitude,elevation (ft),date,ppt\"): return i\n",
    "            if \"name,\" in low and \",date,\" in low: return i\n",
    "    return None\n",
    "\n",
    "def read_one_prism(path: Path) -> pd.DataFrame:\n",
    "    hdr = _find_header_idx(path);  hdr = 9 if hdr is None else hdr\n",
    "    df = pd.read_csv(path, header=hdr, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    if \"Date\" in df.columns:\n",
    "        df = df[df[\"Date\"].astype(str).str.match(r\"\\d{4}-\\d{2}-\\d{2}\", na=False)]\n",
    "    miss = [c for c in EXP if c not in df.columns]\n",
    "    if miss: raise ValueError(f\"{path.name}: missing {miss}\")\n",
    "    for c in EXP:\n",
    "        if c not in [\"Name\",\"Date\"]:\n",
    "            df[c] = df[c].apply(_clean_num)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Date\"]).copy()\n",
    "    df[\"__src\"] = path.name\n",
    "    return df\n",
    "\n",
    "def load_prism_all(root: Path, pattern: str) -> pd.DataFrame:\n",
    "    files = sorted(root.rglob(pattern))\n",
    "    if not files: raise FileNotFoundError(f\"No files under {root.resolve()}\")\n",
    "    parts = [read_one_prism(p) for p in files]\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "    out = out.drop_duplicates(subset=[\"Name\",\"Date\",\"ppt (inches)\",\"tmin (degrees F)\",\n",
    "                                      \"tmean (degrees F)\",\"tmax (degrees F)\",\"tdmean (degrees F)\",\n",
    "                                      \"vpdmin (hPa)\",\"vpdmax (hPa)\"])\n",
    "    return out\n",
    "\n",
    "# --- unit helpers + daily feature engineer (ET/VPD/heat/dry etc.) ---\n",
    "f_to_c     = lambda f: (f-32.0)*(5/9)\n",
    "in_to_mm   = lambda x: x*25.4\n",
    "ft_to_m    = lambda x: x*0.3048\n",
    "hpa_to_kpa = lambda x: x*0.1\n",
    "HEAT_C     = 35.0\n",
    "\n",
    "def build_daily(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = raw.copy()\n",
    "    d[\"Year\"]  = d[\"Date\"].dt.year\n",
    "    d[\"Month\"] = d[\"Date\"].dt.month\n",
    "    d[\"ppt_mm\"]     = in_to_mm(d[\"ppt (inches)\"])\n",
    "    d[\"tmin_C\"]     = f_to_c(d[\"tmin (degrees F)\"])\n",
    "    d[\"tmean_C\"]    = f_to_c(d[\"tmean (degrees F)\"])\n",
    "    d[\"tmax_C\"]     = f_to_c(d[\"tmax (degrees F)\"])\n",
    "    d[\"tdmean_C\"]   = f_to_c(d[\"tdmean (degrees F)\"])\n",
    "    d[\"vpdmin_kPa\"] = hpa_to_kpa(d[\"vpdmin (hPa)\"])\n",
    "    d[\"vpdmax_kPa\"] = hpa_to_kpa(d[\"vpdmax (hPa)\"])\n",
    "    d[\"vpdmean_kPa\"]= d[[\"vpdmin_kPa\",\"vpdmax_kPa\"]].mean(axis=1)\n",
    "    d[\"Elev_m\"]     = ft_to_m(d[\"Elevation (ft)\"])\n",
    "    d[\"HeatDay\"]    = (d[\"tmax_C\"] > HEAT_C).astype(int)\n",
    "    d[\"FrostDay\"]   = (d[\"tmin_C\"] < 0.0).astype(int)\n",
    "    d[\"ThermalAmp\"] = d[\"tmax_C\"] - d[\"tmin_C\"]\n",
    "    d[\"ET0_proxy\"]  = d[\"vpdmean_kPa\"] * (d[\"tmean_C\"] + 5).clip(lower=0)\n",
    "    d[\"P_minus_ET\"] = d[\"ppt_mm\"] - d[\"ET0_proxy\"]\n",
    "    d[\"DryDay\"]     = (d[\"ppt_mm\"] < 1.0).astype(int)\n",
    "    d[\"HeavyRain\"]  = (d[\"ppt_mm\"] > 20.0).astype(int)\n",
    "    d[\"VPD_hi_day\"] = (d[\"vpdmax_kPa\"] > 2.0).astype(int)\n",
    "    d[\"VPD_load\"]   = np.maximum(0.0, d[\"vpdmax_kPa\"] - 2.0)\n",
    "    d[\"DewDep_C\"]   = d[\"tmean_C\"] - d[\"tdmean_C\"]\n",
    "    return d\n",
    "\n",
    "# --- streak helper ---\n",
    "def longest_streak_bool(arr: pd.Series) -> int:\n",
    "    m = 0; cur = 0\n",
    "    for v in arr.astype(bool):\n",
    "        if v: cur += 1; m = max(m, cur)\n",
    "        else: cur = 0\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d1557af-a73e-47a0-a006-8a9201180da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wheat stage rows: 24600 | wide rows: (4100, 104)\n"
     ]
    }
   ],
   "source": [
    "# ------------wheat_pipeline.py------\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- CONFIG --------\n",
    "DATA_DIR = Path(\"Daily_for_transform/Daily Climate Data\")\n",
    "PATTERN  = \"PRISM_ppt_tmin_tmean_tmax_tdmean_vpdmin_vpdmax_stable_800m_*.csv\"\n",
    "OUTDIR   = Path(\"Kashish_results\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "YIELD_CSV = \"crop_yield_1980-2022.csv\"\n",
    "\n",
    "# -------- Wheat specifics --------\n",
    "# GDD base commonly ~4.4 °C (40°F). Heat stress threshold milder than corn; we already mark HeatDay at 35°C.\n",
    "WHEAT_GDD_BASE = 4.4\n",
    "\n",
    "# Stage windows in Days After Emergence (DAE), non-overlapping for features\n",
    "WHEAT_STAGES = [\n",
    "    (\"TILLER\",       0,   60),\n",
    "    (\"STEM_ELONG\",  61,  110),\n",
    "    (\"BOOT\",       111,  135),\n",
    "    (\"HEAD_FLOWER\",136,  155),\n",
    "    (\"GRAIN_FILL\", 156,  190),\n",
    "    (\"RIPEN\",      191,  220),\n",
    "]\n",
    "\n",
    "def detect_wheat_emergence_seasons(daily: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a table of wheat seasons per county with Emergence possibly in the previous fall.\n",
    "    SeasonYear = harvest year. Try Sep 1–Dec 31 of Year-1; fallback Jan 1–Apr 1 of Year.\n",
    "    Criteria: 5-day mean Tmean > 4 °C AND cumulative GDD_base4.4 ≥ 80 within the window.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    names = daily[\"Name\"].unique()\n",
    "    yr_min, yr_max = daily[\"Date\"].dt.year.min(), daily[\"Date\"].dt.year.max()\n",
    "    for name in names:\n",
    "        dn = daily[daily[\"Name\"]==name].sort_values(\"Date\").copy()\n",
    "        for season_year in range(max(yr_min+1, 1983), yr_max+1):\n",
    "            # 1) primary window: previous fall\n",
    "            fall0 = pd.Timestamp(season_year-1, 9, 1)\n",
    "            fall1 = pd.Timestamp(season_year-1, 12, 31)\n",
    "            g = dn[(dn[\"Date\"]>=fall0) & (dn[\"Date\"]<=fall1)].copy()\n",
    "            g[\"GDD4\"] = np.maximum(0.0, g[\"tmean_C\"] - WHEAT_GDD_BASE)\n",
    "            g[\"roll5_Tmean\"] = g[\"tmean_C\"].rolling(5, min_periods=3).mean()\n",
    "            g[\"cumGDD4\"] = g[\"GDD4\"].cumsum()\n",
    "            hit = g[(g[\"roll5_Tmean\"]>4.0) & (g[\"cumGDD4\"]>=80.0)]\n",
    "            if not hit.empty:\n",
    "                emer = hit[\"Date\"].iloc[0]\n",
    "                out.append({\"Name\":name, \"SeasonYear\":season_year, \"Emergence\":emer})\n",
    "                continue\n",
    "            # 2) fallback: early spring of season year\n",
    "            spr0 = pd.Timestamp(season_year, 1, 1)\n",
    "            spr1 = pd.Timestamp(season_year, 4, 1)\n",
    "            g2 = dn[(dn[\"Date\"]>=spr0) & (dn[\"Date\"]<=spr1)].copy()\n",
    "            if g2.empty:\n",
    "                continue\n",
    "            g2[\"GDD4\"] = np.maximum(0.0, g2[\"tmean_C\"] - WHEAT_GDD_BASE)\n",
    "            g2[\"roll5_Tmean\"] = g2[\"tmean_C\"].rolling(5, min_periods=3).mean()\n",
    "            g2[\"cumGDD4\"] = g2[\"GDD4\"].cumsum()\n",
    "            hit2 = g2[(g2[\"roll5_Tmean\"]>4.0) & (g2[\"cumGDD4\"]>=80.0)]\n",
    "            emer2 = hit2[\"Date\"].iloc[0] if not hit2.empty else pd.NaT\n",
    "            out.append({\"Name\":name, \"SeasonYear\":season_year, \"Emergence\":emer2})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def tag_wheat_stages(daily: pd.DataFrame, seasons: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign SeasonYear and Stage by iterating season rows and slicing the daily table.\n",
    "    DAE computed against Emergence; rows outside [Emergence, Emergence+220] are ignored for wheat.\n",
    "    Adds wheat-specific GDD_day (base 4.4 °C) for stage aggregation.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for r in seasons.itertuples(index=False):\n",
    "        if pd.isna(r.Emergence): \n",
    "            continue\n",
    "        lo = r.Emergence\n",
    "        hi = r.Emergence + pd.Timedelta(days=220)\n",
    "        slab = daily[(daily[\"Name\"]==r.Name) & (daily[\"Date\"]>=lo) & (daily[\"Date\"]<=hi)].copy()\n",
    "        if slab.empty:\n",
    "            continue\n",
    "        slab[\"SeasonYear\"] = r.SeasonYear\n",
    "        slab[\"DAE\"] = (slab[\"Date\"] - r.Emergence).dt.days\n",
    "        # wheat-specific GDD\n",
    "        slab[\"GDD_day\"] = np.maximum(0.0, slab[\"tmean_C\"] - WHEAT_GDD_BASE)\n",
    "        # stage label\n",
    "        def stage_from_dae(v):\n",
    "            for lab, lo_d, hi_d in WHEAT_STAGES:\n",
    "                if lo_d <= v <= hi_d: return lab\n",
    "            return None\n",
    "        slab[\"Stage\"] = slab[\"DAE\"].apply(stage_from_dae)\n",
    "        parts.append(slab)\n",
    "    if not parts:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "def wheat_stage_aggregate(tagged: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates the same rich feature set you used for other crops, but by SeasonYear.\n",
    "    \"\"\"\n",
    "    g = tagged.dropna(subset=[\"Stage\"]).copy().sort_values([\"Name\",\"SeasonYear\",\"Stage\",\"Date\"])\n",
    "    # core aggregates\n",
    "    core = (g.groupby([\"Name\",\"SeasonYear\",\"Stage\"], as_index=False)\n",
    "              .agg(\n",
    "                  # thermal\n",
    "                  GDD_sum=(\"GDD_day\",\"sum\"),\n",
    "                  HeatDays=(\"HeatDay\",\"sum\"),\n",
    "                  FrostDays=(\"FrostDay\",\"sum\"),\n",
    "                  Tmean_C=(\"tmean_C\",\"mean\"),\n",
    "                  Tmax_C=(\"tmax_C\",\"mean\"),\n",
    "                  ThermalAmp=(\"ThermalAmp\",\"mean\"),\n",
    "                  # water\n",
    "                  PPT_mm=(\"ppt_mm\",\"sum\"),\n",
    "                  ET0_sum=(\"ET0_proxy\",\"sum\"),\n",
    "                  P_minus_ET=(\"P_minus_ET\",\"sum\"),\n",
    "                  DryDays=(\"DryDay\",\"sum\"),\n",
    "                  HeavyRainDays=(\"HeavyRain\",\"sum\"),\n",
    "                  # VPD / humidity\n",
    "                  VPDmax_kPa=(\"vpdmax_kPa\",\"mean\"),\n",
    "                  VPDmean_kPa=(\"vpdmean_kPa\",\"mean\"),\n",
    "                  VPD_hi_days=(\"VPD_hi_day\",\"sum\"),\n",
    "                  VPD_load_sum=(\"VPD_load\",\"sum\"),\n",
    "                  DewDep_C=(\"DewDep_C\",\"mean\"),\n",
    "              ))\n",
    "    # streaks\n",
    "    grp = g.groupby([\"Name\",\"SeasonYear\",\"Stage\"], observed=True)\n",
    "    streaks = (pd.DataFrame({\n",
    "        \"LongestDryStreak\": grp[\"DryDay\"].apply(longest_streak_bool),\n",
    "        \"LongestHotStreak\": grp[\"HeatDay\"].apply(longest_streak_bool),\n",
    "    }).reset_index())\n",
    "    out = core.merge(streaks, on=[\"Name\",\"SeasonYear\",\"Stage\"], how=\"left\")\n",
    "    out[\"EffPrecRatio_idx\"] = out[\"PPT_mm\"] / out[\"ET0_sum\"].replace(0, np.nan)\n",
    "    return out\n",
    "\n",
    "# ================= RUN =================\n",
    "# 1) Load and daily features (reuse your robust loader and extended daily builder)\n",
    "raw   = load_prism_all(DATA_DIR, PATTERN)\n",
    "daily = build_daily(raw)  # must include ET0_proxy, P_minus_ET, HeatDay, DryDay, VPD*, etc.\n",
    "\n",
    "# 2) Detect wheat seasons and emergence (cross-year)\n",
    "wht_seasons = detect_wheat_emergence_seasons(daily)\n",
    "wht_tagged  = tag_wheat_stages(daily, wht_seasons)\n",
    "\n",
    "# 3) Aggregate by SeasonYear×Stage\n",
    "wht_stage = wheat_stage_aggregate(wht_tagged)\n",
    "\n",
    "# 4) Save long and wide\n",
    "wht_tagged.to_parquet(OUTDIR/\"wheat_daily_with_stages.parquet\", index=False)\n",
    "wht_stage.to_csv(OUTDIR/\"wheat_stage_features.csv\", index=False)\n",
    "\n",
    "# 5) Pivot to wide per Name×SeasonYear; rename SeasonYear -> Year to match yields\n",
    "wide = (wht_stage.pivot_table(\n",
    "    index=[\"Name\",\"SeasonYear\"], columns=\"Stage\",\n",
    "    values=[\"PPT_mm\",\"ET0_sum\",\"P_minus_ET\",\"EffPrecRatio_idx\",\n",
    "            \"GDD_sum\",\"HeatDays\",\"FrostDays\",\"LongestDryStreak\",\"LongestHotStreak\",\n",
    "            \"Tmean_C\",\"Tmax_C\",\"ThermalAmp\",\n",
    "            \"VPDmax_kPa\",\"VPDmean_kPa\",\"VPD_hi_days\",\"VPD_load_sum\",\"DewDep_C\"],\n",
    "    aggfunc=\"first\").reset_index())\n",
    "wide.columns = [\"_\".join(c).strip(\"_\") for c in wide.columns.to_flat_index()]\n",
    "wide = wide.rename(columns={\"SeasonYear\":\"Year\"})\n",
    "wide.to_csv(OUTDIR/\"wheat_stage_features_wide_plus.csv\", index=False)\n",
    "\n",
    "print(\"Wheat stage rows:\", len(wht_stage), \"| wide rows:\", wide.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d26e4024-8509-436b-bb80-f9341bcf3bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Merge yields ≤2022 and prepare 2023 inference\n",
    "yld = pd.read_csv(YIELD_CSV)\n",
    "# Use any wheat entry: includes \"WHEAT\" or \"WHEAT, WINTER\" etc.\n",
    "is_wheat = yld[\"Commodity\"].str.upper().str.contains(\"WHEAT\")\n",
    "yld_wht = (yld.loc[is_wheat, [\"Year\",\"County\",\"Commodity\",\"YIELD_LBS_PER_ACRE\"]]\n",
    "              .rename(columns={\"County\":\"Name\",\"YIELD_LBS_PER_ACRE\":\"Yield\"}))\n",
    "\n",
    "wht_df = (yld_wht.merge(wide[wide[\"Year\"]<=2022], on=[\"Name\",\"Year\"], how=\"inner\")\n",
    "                   .dropna()\n",
    "                   .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "wht_df.to_csv(OUTDIR/\"wheat_model_table.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8aa384f-63c5-48d2-9825-7b4122c77806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model table: (2836, 106) | 2023 inference: (100, 104)\n",
      "Saved: Kashish_results/wheat_predictions_2023.csv\n"
     ]
    }
   ],
   "source": [
    "wht_infer_2023 = (wide[wide[\"Year\"]==2023]\n",
    "                  .dropna()\n",
    "                  .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "wht_infer_2023.to_csv(OUTDIR/\"wheat_inference_2023.csv\", index=False)\n",
    "\n",
    "print(\"Model table:\", wht_df.shape, \"| 2023 inference:\", wht_infer_2023.shape)\n",
    "\n",
    "# 7) Baseline model → 2023 predictions\n",
    "FEATS = [c for c in wht_df.columns if c not in [\"Name\",\"Year\",\"Commodity\",\"Yield\"]]\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, random_state=42\n",
    "    )\n",
    "except Exception:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    model = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "\n",
    "model.fit(wht_df[FEATS], wht_df[\"Yield\"])\n",
    "pred23 = model.predict(wht_infer_2023[FEATS])\n",
    "\n",
    "predictions_wheat = pd.DataFrame({\n",
    "    \"Year\": 2023,\n",
    "    \"Commodity\": \"WHEAT\",\n",
    "    \"Name\": wht_infer_2023[\"Name\"].values,\n",
    "    \"Value\": pred23\n",
    "})\n",
    "predictions_wheat.to_csv(OUTDIR/\"wheat_predictions_2023.csv\", index=False)\n",
    "print(\"Saved:\", OUTDIR/\"wheat_predictions_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9db2b63c-8fd1-42e4-8b8c-61925bbf4059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cotton stage rows: 25200 | wide rows: (4200, 104)\n",
      "Model table: (1541, 106) | 2023 inference: (100, 104)\n"
     ]
    }
   ],
   "source": [
    "# cotton_pipeline.py\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== CONFIG =====\n",
    "DATA_DIR = Path(\"/home/ec2-user/SageMaker/PSI Hackathon/Daily_for_transform/Daily Climate Data\")\n",
    "PATTERN  = \"PRISM_ppt_tmin_tmean_tmax_tdmean_vpdmin_vpdmax_stable_800m_*.csv\"\n",
    "OUTDIR   = Path(\"Kashish_results\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "YIELD_CSV = \"crop_yield_1980-2022.csv\"\n",
    "\n",
    "# ===== COTTON PHENOLOGY =====\n",
    "# Base temp ~15.6°C (60°F). Use for emergence + GDD.\n",
    "COTTON_GDD_BASE = 15.6\n",
    "\n",
    "# Non-overlapping DAE windows for features (NC-typical lengths)\n",
    "COTTON_STAGES = [\n",
    "    (\"VE\",          0,   10),   # emergence\n",
    "    (\"VEG\",        11,   35),   # vegetative expansion\n",
    "    (\"SQUARING\",   36,   55),   # first squares\n",
    "    (\"FLOWERING\",  56,   85),   # first bloom to peak bloom\n",
    "    (\"BOLL_DEV\",   86,  120),   # boll development/fill\n",
    "    (\"OPEN_MAT\",  121,  160),   # open boll to cutout/maturity\n",
    "]\n",
    "\n",
    "# ===== Emergence detection (cotton-specific) =====\n",
    "def detect_emergence_cotton(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Per Name×Year: first date ≥ Mar 1 where roll-5 Tmean > 15°C and cum GDD15.6 ≥ 60.\"\"\"\n",
    "    rows=[]\n",
    "    for (name, yr), g in d.groupby([\"Name\",\"Year\"]):\n",
    "        g = g.sort_values(\"Date\")\n",
    "        g = g[g[\"Date\"] >= pd.Timestamp(yr,3,1)].copy()\n",
    "        if g.empty:\n",
    "            rows.append({\"Name\":name,\"Year\":yr,\"Emergence\":pd.NaT}); continue\n",
    "        g[\"GDD15\"] = np.maximum(0.0, g[\"tmean_C\"] - COTTON_GDD_BASE)\n",
    "        g[\"roll5_Tmean\"] = g[\"tmean_C\"].rolling(5, min_periods=3).mean()\n",
    "        g[\"cumGDD15\"]    = g[\"GDD15\"].cumsum()\n",
    "        hit = g[(g[\"roll5_Tmean\"]>15.0) & (g[\"cumGDD15\"]>=60.0)]\n",
    "        emer = hit[\"Date\"].iloc[0] if not hit.empty else pd.NaT\n",
    "        rows.append({\"Name\":name,\"Year\":yr,\"Emergence\":emer})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ===== Stage tagging =====\n",
    "def tag_cotton_stages(d: pd.DataFrame, emer_tbl: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = d.merge(emer_tbl, on=[\"Name\",\"Year\"], how=\"left\")\n",
    "    m[\"DAE\"] = (m[\"Date\"] - m[\"Emergence\"]).dt.days\n",
    "    # cotton-specific GDD for aggregation\n",
    "    m[\"GDD_day\"] = np.maximum(0.0, m[\"tmean_C\"] - COTTON_GDD_BASE)\n",
    "\n",
    "    def stage_from_dae(v):\n",
    "        if pd.isna(v): return None\n",
    "        for lab, lo, hi in COTTON_STAGES:\n",
    "            if lo <= v <= hi: return lab\n",
    "        return None\n",
    "\n",
    "    m[\"Stage\"] = m[\"DAE\"].apply(stage_from_dae)\n",
    "    return m\n",
    "\n",
    "# ===== Streak helper (reuse if not in scope) =====\n",
    "def longest_streak_bool(arr: pd.Series) -> int:\n",
    "    m = 0; cur = 0\n",
    "    for v in arr.astype(bool):\n",
    "        if v: cur += 1; m = max(m, cur)\n",
    "        else: cur = 0\n",
    "    return m\n",
    "\n",
    "# ===== Aggregate features by Stage =====\n",
    "def cotton_stage_aggregate(tagged: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = tagged.dropna(subset=[\"Stage\"]).copy().sort_values([\"Name\",\"Year\",\"Stage\",\"Date\"])\n",
    "    core = (g.groupby([\"Name\",\"Year\",\"Stage\"], as_index=False)\n",
    "              .agg(\n",
    "                  # thermal\n",
    "                  GDD_sum=(\"GDD_day\",\"sum\"),\n",
    "                  HeatDays=(\"HeatDay\",\"sum\"),\n",
    "                  FrostDays=(\"FrostDay\",\"sum\"),\n",
    "                  Tmean_C=(\"tmean_C\",\"mean\"),\n",
    "                  Tmax_C=(\"tmax_C\",\"mean\"),\n",
    "                  ThermalAmp=(\"ThermalAmp\",\"mean\"),\n",
    "                  # water\n",
    "                  PPT_mm=(\"ppt_mm\",\"sum\"),\n",
    "                  ET0_sum=(\"ET0_proxy\",\"sum\"),\n",
    "                  P_minus_ET=(\"P_minus_ET\",\"sum\"),\n",
    "                  DryDays=(\"DryDay\",\"sum\"),\n",
    "                  HeavyRainDays=(\"HeavyRain\",\"sum\"),\n",
    "                  # VPD / humidity\n",
    "                  VPDmax_kPa=(\"vpdmax_kPa\",\"mean\"),\n",
    "                  VPDmean_kPa=(\"vpdmean_kPa\",\"mean\"),\n",
    "                  VPD_hi_days=(\"VPD_hi_day\",\"sum\"),\n",
    "                  VPD_load_sum=(\"VPD_load\",\"sum\"),\n",
    "                  DewDep_C=(\"DewDep_C\",\"mean\"),\n",
    "              ))\n",
    "    grp = g.groupby([\"Name\",\"Year\",\"Stage\"], observed=True)\n",
    "    streaks = (pd.DataFrame({\n",
    "        \"LongestDryStreak\": grp[\"DryDay\"].apply(longest_streak_bool),\n",
    "        \"LongestHotStreak\": grp[\"HeatDay\"].apply(longest_streak_bool),\n",
    "    }).reset_index())\n",
    "    out = core.merge(streaks, on=[\"Name\",\"Year\",\"Stage\"], how=\"left\")\n",
    "    out[\"EffPrecRatio_idx\"] = out[\"PPT_mm\"] / out[\"ET0_sum\"].replace(0, np.nan)\n",
    "    return out\n",
    "\n",
    "# ===== RUN =====\n",
    "# Requires you already have: load_prism_all(DATA_DIR, PATTERN), build_daily(raw)\n",
    "raw   = load_prism_all(DATA_DIR, PATTERN)\n",
    "daily = build_daily(raw)\n",
    "\n",
    "emer  = detect_emergence_cotton(daily)\n",
    "cot_tagged = tag_cotton_stages(daily, emer)\n",
    "cot_stage  = cotton_stage_aggregate(cot_tagged)\n",
    "\n",
    "# save long\n",
    "cot_tagged.to_parquet(OUTDIR/\"cotton_daily_with_stages.parquet\", index=False)\n",
    "cot_stage.to_csv(OUTDIR/\"cotton_stage_features.csv\", index=False)\n",
    "\n",
    "# wide per Name×Year\n",
    "wide = (cot_stage.pivot_table(\n",
    "    index=[\"Name\",\"Year\"], columns=\"Stage\",\n",
    "    values=[\"PPT_mm\",\"ET0_sum\",\"P_minus_ET\",\"EffPrecRatio_idx\",\n",
    "            \"GDD_sum\",\"HeatDays\",\"FrostDays\",\"LongestDryStreak\",\"LongestHotStreak\",\n",
    "            \"Tmean_C\",\"Tmax_C\",\"ThermalAmp\",\n",
    "            \"VPDmax_kPa\",\"VPDmean_kPa\",\"VPD_hi_days\",\"VPD_load_sum\",\"DewDep_C\"],\n",
    "    aggfunc=\"first\").reset_index())\n",
    "wide.columns = [\"_\".join(c).strip(\"_\") for c in wide.columns.to_flat_index()]\n",
    "wide.to_csv(OUTDIR/\"cotton_stage_features_wide_plus.csv\", index=False)\n",
    "\n",
    "print(\"Cotton stage rows:\", len(cot_stage), \"| wide rows:\", wide.shape)\n",
    "\n",
    "# ===== Merge yields ≤2022 and prepare 2023 inference =====\n",
    "yld = pd.read_csv(YIELD_CSV)\n",
    "\n",
    "# keep upland cotton yields in lb/acre\n",
    "is_cotton = yld[\"Commodity\"].str.upper().str.contains(\"COTTON\")\n",
    "yld_cot = (yld.loc[is_cotton, [\"Year\",\"County\",\"Commodity\",\"YIELD_LBS_PER_ACRE\"]]\n",
    "              .rename(columns={\"County\":\"Name\",\"YIELD_LBS_PER_ACRE\":\"Yield\"}))\n",
    "\n",
    "cot_df = (yld_cot.merge(wide[wide[\"Year\"]<=2022], on=[\"Name\",\"Year\"], how=\"inner\")\n",
    "                 .dropna()\n",
    "                 .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "cot_df.to_csv(OUTDIR/\"cotton_model_table.csv\", index=False)\n",
    "\n",
    "cot_infer_2023 = (wide[wide[\"Year\"]==2023]\n",
    "                  .dropna()\n",
    "                  .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "cot_infer_2023.to_csv(OUTDIR/\"cotton_inference_2023.csv\", index=False)\n",
    "\n",
    "print(\"Model table:\", cot_df.shape, \"| 2023 inference:\", cot_infer_2023.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfcfaa-141d-418a-9150-5784f43ed72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Baseline model → 2023 predictions =====\n",
    "FEATS = [c for c in cot_df.columns if c not in [\"Name\",\"Year\",\"Commodity\",\"Yield\"]]\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=600, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, random_state=42\n",
    "    )\n",
    "except Exception:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    model = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "\n",
    "model.fit(cot_df[FEATS], cot_df[\"Yield\"])\n",
    "pred23 = model.predict(cot_infer_2023[FEATS])\n",
    "\n",
    "predictions_cotton = pd.DataFrame({\n",
    "    \"Year\": 2023,\n",
    "    \"Commodity\": \"COTTON\",\n",
    "    \"Name\": cot_infer_2023[\"Name\"].values,\n",
    "    \"Value\": pred23\n",
    "})\n",
    "predictions_cotton.to_csv(OUTDIR/\"cotton_predictions_2023.csv\", index=False)\n",
    "print(\"Saved:\", OUTDIR/\"cotton_predictions_2023.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4382b2c8-d108-421c-ba98-c2d6f21c361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to: /home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/corn_inference\n",
      "Saved: /home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/corn_inference/corn_inference_2023.csv | shape: (100, 104)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ensure OUTDIR exists and show where we're writing\n",
    "OUTDIR = Path(\"Kashish_results/corn\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Writing to:\", OUTDIR.resolve())\n",
    "\n",
    "corn_infer_2023 = (\n",
    "    wide.loc[wide[\"Year\"]==2023]\n",
    "        .drop_duplicates(subset=[\"Name\",\"Year\"])\n",
    "        .copy()\n",
    ")\n",
    "\n",
    "out_file = OUTDIR / \"corn_inference_2023.csv\"\n",
    "corn_infer_2023.to_csv(out_file, index=False)\n",
    "print(\"Saved:\", out_file.resolve(), \"| shape:\", corn_infer_2023.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87c2c159-0359-4fd9-90f4-f3e236496dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winter wheat rows: 3313 years: 1980 - 2022\n",
      "Climate-only (no winter yield) rows: 1064\n",
      "Model table (winter-only): (2836, 107)\n",
      "         Name  Year\n",
      "64  ALEXANDER  2009\n",
      "65  ALEXANDER  2010\n",
      "68  ALEXANDER  2013\n",
      "71  ALEXANDER  2016\n",
      "72  ALEXANDER  2017\n",
      "77  ALEXANDER  2022\n",
      "80  ALLEGHANY  1986\n",
      "81  ALLEGHANY  1987\n",
      "82  ALLEGHANY  1988\n",
      "83  ALLEGHANY  1989\n",
      "84  ALLEGHANY  1990\n",
      "85  ALLEGHANY  1991\n",
      "86  ALLEGHANY  1992\n",
      "87  ALLEGHANY  1993\n",
      "88  ALLEGHANY  1994\n",
      "89  ALLEGHANY  1995\n",
      "90  ALLEGHANY  1996\n",
      "94  ALLEGHANY  2000\n",
      "95  ALLEGHANY  2001\n",
      "96  ALLEGHANY  2002\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUTDIR = Path(\"Kashish_results\"); OUTDIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# 1) Load yields and KEEP ONLY winter wheat BU/acre\n",
    "WINTER_ITEM = \"WHEAT, WINTER - YIELD, MEASURED IN BU / ACRE\"\n",
    "yld = pd.read_csv(\"crop_yield_1980-2022.csv\")\n",
    "\n",
    "w_winter = (yld.loc[yld[\"Data Item\"].eq(WINTER_ITEM),\n",
    "                    [\"Year\",\"County\",\"Commodity\",\"Data Item\",\"YIELD_LBS_PER_ACRE\"]]\n",
    "              .rename(columns={\"County\":\"Name\",\"YIELD_LBS_PER_ACRE\":\"Yield\"}))\n",
    "\n",
    "print(\"Winter wheat rows:\", len(w_winter), \"years:\", w_winter[\"Year\"].min(), \"-\", w_winter[\"Year\"].max())\n",
    "\n",
    "# 2) Climate features (wide) and restrict to ≤2022\n",
    "wide = pd.read_csv(\"/home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/wheat/wheat_stage_features_wide_plus.csv\")\n",
    "wide_learn = wide[wide[\"Year\"] <= 2022].copy()\n",
    "\n",
    "# 3) Coverage diagnostics: which Name–Year in climate lack winter-wheat yield\n",
    "clim_keys = wide_learn[[\"Name\",\"Year\"]].drop_duplicates()\n",
    "yld_keys  = w_winter[[\"Name\",\"Year\"]].drop_duplicates()\n",
    "\n",
    "missing_yield = (clim_keys.merge(yld_keys, on=[\"Name\",\"Year\"], how=\"left\", indicator=True)\n",
    "                         .query(\"_merge=='left_only'\")\n",
    "                         .drop(columns=\"_merge\"))\n",
    "missing_yield.to_csv(OUTDIR/\"wheat_winter_missing_name_year.csv\", index=False)\n",
    "print(\"Climate-only (no winter yield) rows:\", len(missing_yield))\n",
    "\n",
    "# 4) Merge ONLY winter-wheat yields with climate features\n",
    "wht_df = (w_winter.merge(wide_learn, on=[\"Name\",\"Year\"], how=\"inner\")\n",
    "                   .dropna()\n",
    "                   .drop_duplicates(subset=[\"Name\",\"Year\"]))\n",
    "\n",
    "# keep Data Item in the model table\n",
    "wht_df.to_csv(OUTDIR/\"wheat_model_table_winter_only.csv\", index=False)\n",
    "print(\"Model table (winter-only):\", wht_df.shape)\n",
    "\n",
    "# 5) Optional: show a few missing cases\n",
    "print(missing_yield.sort_values([\"Name\",\"Year\"]).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7f910748-76c5-48fb-b93e-7be27b38a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative yields found for missing winter-wheat rows: 0\n",
      "Series([], Name: Name, dtype: int64)\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Program, Year, Period, Week Ending, Geo Level, State, State ANSI, Ag District, Ag District Code, Name, County ANSI, Zip Code, Region, watershed_code, Watershed, Commodity, Data Item, Domain, Domain Category, Value, CV (%), Yield]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUTDIR = Path(\"Kashish_results\")\n",
    "\n",
    "# load the full yield dataset\n",
    "yld = pd.read_csv(\"crop_yield_1980-2022.csv\")\n",
    "\n",
    "# define items of interest\n",
    "all_items = [\n",
    "    \"WHEAT - YIELD, MEASURED IN BU / ACRE\",\n",
    "    \"WHEAT - YIELD, MEASURED IN BU / NET PLANTED ACRE\",\n",
    "    \"WHEAT, WINTER - YIELD, MEASURED IN BU / ACRE\",\n",
    "    \"WHEAT, WINTER - YIELD, MEASURED IN BU / NET PLANTED ACRE\"\n",
    "]\n",
    "\n",
    "wheat_all = yld[yld[\"Data Item\"].isin(all_items)].copy()\n",
    "wheat_all = wheat_all.rename(columns={\"County\":\"Name\",\"YIELD_LBS_PER_ACRE\":\"Yield\"})\n",
    "\n",
    "# load the missing Name-Year list (from your previous step)\n",
    "missing = pd.read_csv(OUTDIR/\"wheat_winter_missing_name_year.csv\")\n",
    "\n",
    "# check which of the other items are available for those missing\n",
    "alts = (wheat_all.merge(missing, on=[\"Name\",\"Year\"], how=\"inner\")\n",
    "                  .sort_values([\"Name\",\"Year\",\"Data Item\"]))\n",
    "\n",
    "alts.to_csv(OUTDIR/\"wheat_missing_winter_with_other_items.csv\", index=False)\n",
    "\n",
    "print(\"Alternative yields found for missing winter-wheat rows:\", len(alts))\n",
    "print(alts.groupby(\"Data Item\")[\"Name\"].count())\n",
    "print(alts.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae42d5b9-991e-488f-a90b-2e30408e1637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model rows: 3327\n",
      "Silage yields matched: 823\n",
      "Missing silage yields: 2504\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# paths\n",
    "model_fp = \"/home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/corn/corn_model_table.csv\"     # has: Year, Name, Yield\n",
    "usda_fp  = \"/home/ec2-user/SageMaker/PSI Hackathon/crop_yield_1980-2022.csv\"        # has: Year, County, Data Item, Value or YIELD_LBS_PER_ACRE\n",
    "\n",
    "# load\n",
    "model = pd.read_csv(model_fp)\n",
    "usda  = pd.read_csv(usda_fp)\n",
    "\n",
    "# normalize county keys\n",
    "def norm_key(s):\n",
    "    return (s.astype(str).str.upper()\n",
    "            .str.replace(r\"\\s+COUNTY$\", \"\", regex=True)\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip())\n",
    "\n",
    "model[\"CountyKey\"] = norm_key(model[\"Name\"])\n",
    "if \"County\" not in usda.columns:\n",
    "    raise ValueError(\"USDA file must contain a 'County' column.\")\n",
    "usda[\"CountyKey\"] = norm_key(usda[\"County\"])\n",
    "\n",
    "# --- select silage rows ---\n",
    "mask_silage = usda[\"Data Item\"].str.contains(\"CORN, SILAGE - YIELD, MEASURED IN TONS / ACRE\", na=False)\n",
    "\n",
    "sil = usda.loc[mask_silage, [\"Year\",\"CountyKey\"]].copy()\n",
    "\n",
    "# get numeric yield for silage (lbs/acre)\n",
    "if \"Value\" in usda.columns:\n",
    "    # Value likely in tons/acre -> convert to lbs/acre\n",
    "    val = (usda.loc[mask_silage, \"Value\"]\n",
    "              .astype(str).str.replace(\",\", \"\", regex=False)\n",
    "              .str.extract(r\"([-+]?\\d*\\.?\\d+)\")[0].astype(float))\n",
    "    sil[\"Silage_Yield_LBS_PER_ACRE\"] = val * 2000.0\n",
    "elif \"YIELD_LBS_PER_ACRE\" in usda.columns:\n",
    "    # already provided in lbs/acre\n",
    "    val = (usda.loc[mask_silage, \"YIELD_LBS_PER_ACRE\"]\n",
    "              .astype(str).str.replace(\",\", \"\", regex=False)\n",
    "              .str.extract(r\"([-+]?\\d*\\.?\\d+)\")[0].astype(float))\n",
    "    sil[\"Silage_Yield_LBS_PER_ACRE\"] = val\n",
    "else:\n",
    "    raise ValueError(\"USDA file needs either 'Value' or 'YIELD_LBS_PER_ACRE'.\")\n",
    "\n",
    "# de-duplicate if multiple entries per Year×County\n",
    "sil = (sil.groupby([\"Year\",\"CountyKey\"], as_index=False)\n",
    "          .agg({\"Silage_Yield_LBS_PER_ACRE\":\"mean\"}))\n",
    "\n",
    "# merge into model (keeps your existing grain Yield column)\n",
    "out = model.merge(sil, how=\"left\", on=[\"Year\",\"CountyKey\"]).drop(columns=[\"CountyKey\"])\n",
    "\n",
    "# report coverage\n",
    "matched = out[\"Silage_Yield\"].notna().sum()\n",
    "print(f\"Model rows: {len(out)}\")\n",
    "print(f\"Silage yields matched: {matched}\")\n",
    "print(f\"Missing silage yields: {len(out)-matched}\")\n",
    "\n",
    "# save\n",
    "out.to_csv(\"Kashish_results/corn/corn_model_with_silage.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7901cea0-9fbb-4404-b1b1-87c1e349b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CORN] rows=3327 cols=158\n",
      "[SOYBEANS] rows=3096 cols=123\n",
      "[PEANUTS] rows=938 cols=106\n",
      "[COTTON] rows=1541 cols=106\n",
      "[WHEAT] ERROR: [Errno 2] No such file or directory: '/home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/wheat/wheat_model_table_winter_only.csv'\n",
      "EDA complete. See: /home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/eda\n"
     ]
    }
   ],
   "source": [
    "# eda_all_crops.py\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "IN = {\n",
    "    \"CORN\":     \"/home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/corn/corn_model_with_silage.csv\",\n",
    "    \"SOYBEANS\": \"/home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/soybean/soybean_model_table.csv\",\n",
    "    \"PEANUTS\":  \"/home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/peanuts/peanut_model_table.csv\",\n",
    "    \"COTTON\":   \"/home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/cotton/cotton_model_table.csv\",\n",
    "    \"WHEAT\":    \"/home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/wheat/wheat_model_table_winter_only.csv\",  # adjust if different\n",
    "}\n",
    "OUTROOT = Path(\"Kashish_results/eda\"); OUTROOT.mkdir(parents=True, exist_ok=True)\n",
    "TARGET = \"Yield\"\n",
    "DROP   = {\"Name\",\"Commodity\"}  # dropped from features if present\n",
    "\n",
    "def safe_num_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    num = df.select_dtypes(include=[np.number]).copy()\n",
    "    # remove columns with zero variance\n",
    "    nunique = num.nunique()\n",
    "    keep = nunique[nunique > 1].index\n",
    "    return num[keep]\n",
    "\n",
    "def eda_one(crop: str, path: str):\n",
    "    outdir = OUTROOT / crop; outdir.mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"[{crop}] rows={len(df)} cols={df.shape[1]}\")\n",
    "\n",
    "    # ---------- 1) Yield over time + distribution ----------\n",
    "    plt.figure(figsize=(11,4))\n",
    "    \n",
    "    sns.lineplot(data=df, x=\"Year\", y=TARGET, estimator=\"mean\", errorbar=None)\n",
    "\n",
    "    plt.title(f\"{crop}: Mean Yield over Time\"); plt.ylabel(\"Yield (lbs/acre)\")\n",
    "    plt.tight_layout(); plt.savefig(outdir/\"yield_over_time.png\", dpi=160); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.histplot(df[TARGET], bins=40, kde=True)\n",
    "    plt.title(f\"{crop}: Yield Distribution\")\n",
    "    plt.tight_layout(); plt.savefig(outdir/\"yield_hist.png\", dpi=160); plt.close()\n",
    "\n",
    "    # ---------- 2) Correlations ----------\n",
    "    num = safe_num_df(df)\n",
    "    if TARGET not in num.columns:\n",
    "        num[TARGET] = df[TARGET].values\n",
    "    corr = num.corr()\n",
    "\n",
    "    # save full correlation matrix\n",
    "    corr.to_csv(outdir/\"corr_matrix.csv\")\n",
    "\n",
    "    # heatmap vs target\n",
    "    ct = corr[[TARGET]].sort_values(by=TARGET, ascending=False)\n",
    "    plt.figure(figsize=(6, min(0.35*len(ct), 16)))\n",
    "    sns.heatmap(ct, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, cbar=False)\n",
    "    plt.title(f\"{crop}: Correlation with Yield\")\n",
    "    plt.tight_layout(); plt.savefig(outdir/\"corr_with_yield.png\", dpi=160); plt.close()\n",
    "\n",
    "    # ---------- 3) Scatterplots of top correlated ----------\n",
    "    top_feats = (ct.index.drop(TARGET)\n",
    "                   .to_series().reindex(ct.index.drop(TARGET))\n",
    "    )\n",
    "    top5 = (corr[TARGET].drop(TARGET).abs().sort_values(ascending=False).head(5).index.tolist())\n",
    "    for feat in top5:\n",
    "        plt.figure(figsize=(5.5,4))\n",
    "        sns.scatterplot(data=df, x=feat, y=TARGET, alpha=0.5)\n",
    "        plt.title(f\"{crop}: Yield vs {feat}\")\n",
    "        plt.tight_layout(); plt.savefig(outdir/f\"scatter_{feat}.png\", dpi=160); plt.close()\n",
    "\n",
    "    pd.Series(top5, name=\"Top5_by_abs_corr\").to_csv(outdir/\"top5_corr_features.csv\", index=False)\n",
    "\n",
    "    # ---------- 4) Feature importance (RF) ----------\n",
    "    X = df.drop(columns=[c for c in [TARGET, *DROP] if c in df.columns], errors=\"ignore\")\n",
    "    # keep only numeric columns\n",
    "    X = X.select_dtypes(include=[np.number]).copy()\n",
    "    # remove zero-variance\n",
    "    nunique = X.nunique()\n",
    "    X = X.loc[:, nunique[nunique>1].index]\n",
    "    y = df[TARGET].values\n",
    "\n",
    "    if X.shape[1] >= 2:\n",
    "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "        rf.fit(Xtr, ytr)\n",
    "        imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        imp.to_csv(outdir/\"rf_feature_importances.csv\")\n",
    "        top15 = imp.head(15)\n",
    "\n",
    "        plt.figure(figsize=(9, max(4, 0.35*len(top15))))\n",
    "        sns.barplot(x=top15.values, y=top15.index)\n",
    "        plt.title(f\"{crop}: Top RF Feature Importances\")\n",
    "        plt.xlabel(\"Importance\"); plt.ylabel(\"\")\n",
    "        plt.tight_layout(); plt.savefig(outdir/\"rf_feature_importances_top15.png\", dpi=160); plt.close()\n",
    "    else:\n",
    "        print(f\"[{crop}] Skipped RF importance (insufficient numeric features).\")\n",
    "\n",
    "for crop, path in IN.items():\n",
    "    try:\n",
    "        eda_one(crop, path)\n",
    "    except Exception as e:\n",
    "        print(f\"[{crop}] ERROR: {e}\")\n",
    "\n",
    "print(\"EDA complete. See:\", OUTROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1fbb3af9-ac20-4c7b-a57e-b21fa55b8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set these if not already\n",
    "crop = \"CORN\"\n",
    "best_col = \"VPDmean_kPa_R5\"\n",
    "\n",
    "# byyr must already exist with Year, Yield_z, Stress_z\n",
    "bad = byyr[(byyr[\"Yield_z\"] <= -0.5) & (byyr[\"Stress_z\"] >= 0.5)]\n",
    "\n",
    "outdir = Path(f\"Kashish_results/stress_analysis_v2/{crop}\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(byyr[\"Year\"], byyr[\"Yield_z\"], color=\"blue\", label=\"Yield (z)\")\n",
    "plt.plot(byyr[\"Year\"], byyr[\"Stress_z\"], color=\"red\", linestyle=\"--\", label=f\"{best_col} (z)\")\n",
    "plt.axhline(0, color=\"gray\", linewidth=0.8)\n",
    "for yr in bad[\"Year\"]:\n",
    "    plt.axvspan(yr-0.5, yr+0.5, color=\"orange\", alpha=0.2)\n",
    "\n",
    "plt.title(f\"{crop}: Yield vs {best_col} (z-scores)\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"z-score\"); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(outdir / f\"yearly_zscores_better_{best_col}.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "bad[[\"Year\",\"Yield_z\",\"Stress_z\"]].to_csv(outdir / f\"bad_years_{best_col}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b60a5c84-3203-481a-a796-19a51912b5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CORN] Stages found: R1, R2, R3, R4, R5, R6, V6, VE, VT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16293/2466995035.py:133: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  cm_long = corr_mat.stack(dropna=True).rename(\"corr\").reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CORN] Top harmful: VPDmean_kPa_R5 r=-0.311  |  stages={'R1': 10, 'R2': 10, 'R3': 10, 'R4': 10, 'R5': 10, 'R6': 10, 'V6': 10, 'VE': 10, 'VT': 10}  |  → Kashish_results/stress_analysis_v3/CORN\n",
      "Done → /home/ec2-user/SageMaker/PSI Hackathon/Kashish_results/stress_analysis_v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16293/2466995035.py:173: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  by_cty = (df.groupby(id_cols, as_index=False).apply(_corr)\n"
     ]
    }
   ],
   "source": [
    "# stress_finder_all_crops.py\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ===== CONFIG =====\n",
    "IN = {\n",
    "    \"CORN\":     \"final_dataset/Corn_normalized.csv\",\n",
    "  #  \"SOYBEANS\": \"final_dataset/Soybean_normalized.csv\",\n",
    "   # \"PEANUTS\":  \"final_dataset/Peanut_normalized.csv\",\n",
    "    #\"COTTON\":   \"final_dataset/Cotton_normalized.csv\",\n",
    "    #\"WHEAT\":    \"final_dataset/Wheat_normalized.csv\",\n",
    "}\n",
    "OUTROOT = Path(\"Kashish_results/stress_analysis_v3\"); OUTROOT.mkdir(parents=True, exist_ok=True)\n",
    "TARGET = \"Yield\"\n",
    "\n",
    "STRESS_PREFIXES = {\n",
    "    # include only stress-like metrics\n",
    "    \"COMMON\": [\"HeatDays\",\"LongestHotStreak\",\"DryDays\",\"LongestDryStreak\",\n",
    "               \"P_minus_ET\",\"EffPrecRatio_idx\",\n",
    "               \"VPD_hi_days\",\"VPD_load_sum\",\"VPDmax_kPa\",\"VPDmean_kPa\",\n",
    "               \"FrostDays\"]\n",
    "}\n",
    "\n",
    "STAGE_TOKENS = {\n",
    "    \"CORN\":     [\"VE\",\"V6\",\"VT\",\"R1\",\"R2\",\"R3\",\"R4\",\"R5\",\"R6\"],\n",
    "    \"SOYBEANS\": [\"VE\",\"V2_V3\",\"V4_V6\",\"R1_R2\",\"R3_R4\",\"R5_R6\",\"R7_R8\"],\n",
    "    \"PEANUTS\":  [\"VE\",\"VEG\",\"PEG\",\"POD_SET\",\"SEED_FILL\",\"MATURITY\"],   # adjust to your columns if needed\n",
    "    \"COTTON\":   [\"VE\",\"VEG\",\"SQUARING\",\"FLOWERING\",\"BOLL_DEV\",\"OPEN_MAT\"],\n",
    "    \"WHEAT\":    [\"TILLER\",\"STEM_ELONG\",\"BOOT\",\"HEAD_FLOWER\",\"GRAIN_FILL\",\"RIPEN\"],\n",
    "}\n",
    "\n",
    "MIN_NEG = -0.10\n",
    "TOP_K   = 5\n",
    "\n",
    "# ===== HELPERS =====\n",
    "def zscore(s: pd.Series):\n",
    "    v = (s - s.mean())/s.std(ddof=0)\n",
    "    return v.replace([np.inf,-np.inf], np.nan)\n",
    "\n",
    "def split_by_stage_suffix(col: str, crop: str):\n",
    "    \"\"\"Robust split: if col endswith '_<stage>' for any known stage token, use that.\n",
    "       Else fallback to regex last-underscore split.\"\"\"\n",
    "    for st in STAGE_TOKENS.get(crop, []):\n",
    "        suf = \"_\"+st\n",
    "        if col.endswith(suf):\n",
    "            return col[:-len(suf)], st\n",
    "    m = re.match(r\"^(.*)_([A-Z0-9]+(?:_[A-Z0-9]+)?)$\", col)\n",
    "    return m.groups() if m else (None, None)\n",
    "\n",
    "def stage_feature_index(df: pd.DataFrame, crop: str, target: str):\n",
    "    rows=[]\n",
    "    skip = {target,\"Year\",\"Name\",\"COUNTY\",\"Commodity\",\"Data Item\",\"Unnamed: 0\"}\n",
    "    prefixes = STRESS_PREFIXES[\"COMMON\"]\n",
    "    for c in df.columns:\n",
    "        if c in skip: \n",
    "            continue\n",
    "        metric, stage = split_by_stage_suffix(c, crop)\n",
    "        if not metric or not stage:\n",
    "            continue\n",
    "        if any(metric.startswith(p) for p in prefixes):\n",
    "            rows.append((c, metric, stage))\n",
    "    return pd.DataFrame(rows, columns=[\"col\",\"metric\",\"stage\"])\n",
    "\n",
    "def nice_stage_order(stages, crop):\n",
    "    pref = STAGE_TOKENS.get(crop, [])\n",
    "    seen = [s for s in pref if s in stages]\n",
    "    rest = [s for s in stages if s not in pref]\n",
    "    return seen + rest\n",
    "\n",
    "# ===== PLOTTING =====\n",
    "def heatmap_corr(corr_mat: pd.DataFrame, title: str, out_png: Path):\n",
    "    plt.figure(figsize=(1.2*corr_mat.shape[1]+2, 0.55*corr_mat.shape[0]+2))\n",
    "    sns.heatmap(corr_mat, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0,\n",
    "                linewidths=.5, linecolor=\"white\", cbar_kws={\"shrink\":0.8})\n",
    "    plt.title(title); plt.tight_layout(); plt.savefig(out_png, dpi=200); plt.close()\n",
    "\n",
    "def scatter_with_fit(df, xcol, ycol, title, out_png):\n",
    "    plt.figure(figsize=(6.4,4.6))\n",
    "    sns.scatterplot(data=df, x=xcol, y=ycol, alpha=0.35, edgecolor=None)\n",
    "    sns.regplot(data=df, x=xcol, y=ycol, scatter=False, color=\"black\")\n",
    "    plt.title(title); plt.tight_layout(); plt.savefig(out_png, dpi=200); plt.close()\n",
    "\n",
    "def dual_z_trend(byyr, crop, best_col, out_png):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(byyr[\"Year\"], byyr[\"Yield_z\"],  color=\"blue\", label=\"Yield (z)\")\n",
    "    plt.plot(byyr[\"Year\"], byyr[\"Stress_z\"], color=\"red\",  linestyle=\"--\", label=f\"{best_col} (z)\")\n",
    "    plt.axhline(0, color=\"gray\", linewidth=0.8)\n",
    "    bad = byyr[(byyr[\"Yield_z\"]<=-0.5) & (byyr[\"Stress_z\"]>=0.5)]\n",
    "    for yr in bad[\"Year\"]:\n",
    "        plt.axvspan(yr-0.5, yr+0.5, color=\"orange\", alpha=0.2)\n",
    "    plt.title(f\"{crop}: Yield vs {best_col} (z-scores)\")\n",
    "    plt.xlabel(\"Year\"); plt.ylabel(\"z-score\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=200); plt.close()\n",
    "    return bad\n",
    "\n",
    "# ===== CORE =====\n",
    "def analyze_crop(crop: str, path: str):\n",
    "    outdir = OUTROOT / crop; outdir.mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # IDs present?\n",
    "    id_cols = [c for c in [\"Name\",\"COUNTY\"] if c in df.columns]\n",
    "    key_cols = [\"Year\", TARGET] + id_cols\n",
    "\n",
    "    # index stress-stage columns using robust splitter\n",
    "    sf = stage_feature_index(df, crop, TARGET)\n",
    "    if sf.empty:\n",
    "        print(f\"[{crop}] No stress-stage columns detected. Check column names.\")\n",
    "        return\n",
    "\n",
    "    # diagnostics to catch “only VE”\n",
    "    stage_counts = sf[\"stage\"].value_counts().sort_index()\n",
    "    stage_counts.to_csv(outdir/\"_debug_stage_counts.csv\")\n",
    "    print(f\"[{crop}] Stages found:\", \", \".join(stage_counts.index.tolist()))\n",
    "\n",
    "    use_cols = key_cols + sf[\"col\"].tolist()\n",
    "    df = df[use_cols].dropna(subset=[TARGET]).copy()\n",
    "\n",
    "    # 1) correlation heatmap\n",
    "    stages = nice_stage_order(sorted(sf[\"stage\"].unique()), crop)\n",
    "    metrics = sorted(sf[\"metric\"].unique())\n",
    "    corr_mat = pd.DataFrame(index=metrics, columns=stages, dtype=float)\n",
    "    for met in metrics:\n",
    "        for st in stages:\n",
    "            col = f\"{met}_{st}\"\n",
    "            corr_mat.loc[met, st] = df[[TARGET, col]].corr().iloc[0,1] if col in df.columns else np.nan\n",
    "    corr_mat.to_csv(outdir/\"corr_stress_stage_vs_yield.csv\")\n",
    "    heatmap_corr(corr_mat, f\"{crop}: Correlation (Stress×Stage) vs Yield\", outdir/\"corr_heatmap.png\")\n",
    "\n",
    "    # 2) top harmful (most negative)\n",
    "    cm_long = corr_mat.stack(dropna=True).rename(\"corr\").reset_index()\n",
    "    cm_long.columns = [\"metric\",\"stage\",\"corr\"]\n",
    "    neg = cm_long[cm_long[\"corr\"] < 0]\n",
    "    if neg.empty:\n",
    "        top = cm_long.sort_values(\"corr\").head(1)\n",
    "    else:\n",
    "        base = neg[neg[\"corr\"] <= MIN_NEG] if (neg[\"corr\"] <= MIN_NEG).any() else neg\n",
    "        top = base.sort_values(\"corr\").head(1)\n",
    "\n",
    "    topk = (neg.sort_values(\"corr\").head(TOP_K) if not neg.empty\n",
    "            else cm_long.sort_values(\"corr\").head(TOP_K))\n",
    "    topk.to_csv(outdir/\"top5_harmful_stress.csv\", index=False)\n",
    "\n",
    "    \n",
    "    metric, stage, corr_val = top.iloc[0][\"metric\"], top.iloc[0][\"stage\"], float(top.iloc[0][\"corr\"])\n",
    "    best_col = f\"{metric}_{stage}\"\n",
    "    pd.Series({\"metric\":metric,\"stage\":stage,\"corr\":corr_val}).to_csv(outdir/\"top_harmful_stress.csv\")\n",
    "\n",
    "    # 3) yearly stress vs yield\n",
    "    byyr = (df.groupby(\"Year\", as_index=False)\n",
    "              .agg(Yield_mean=(TARGET,\"mean\"),\n",
    "                   Stress_mean=(best_col,\"mean\")))\n",
    "    byyr[\"Yield_z\"]  = zscore(byyr[\"Yield_mean\"])\n",
    "    byyr[\"Stress_z\"] = zscore(byyr[\"Stress_mean\"])\n",
    "    byyr.to_csv(outdir/\"year_trends_top_stress.csv\", index=False)\n",
    "\n",
    "    # improved plot + bad years\n",
    "    bad = dual_z_trend(byyr, crop, best_col, outdir/\"yearly_zscores_better.png\")\n",
    "    bad.to_csv(outdir/\"years_high_stress_low_yield.csv\", index=False)\n",
    "\n",
    "    # 4) scatter\n",
    "    scatter_with_fit(df, best_col, TARGET,\n",
    "                     f\"{crop}: Yield vs {best_col} (r={corr_val:.2f})\",\n",
    "                     outdir/f\"scatter_{best_col}.png\")\n",
    "\n",
    "    # optional per-county correlation\n",
    "    if id_cols:\n",
    "        def _corr(g):\n",
    "            if g[TARGET].nunique()<3 or g[best_col].nunique()<3: return np.nan\n",
    "            return g[[TARGET, best_col]].corr().iloc[0,1]\n",
    "        by_cty = (df.groupby(id_cols, as_index=False).apply(_corr)\n",
    "                    .rename(columns={None:\"corr\"}))\n",
    "        by_cty.to_csv(outdir/\"county_corr_with_top_stress.csv\", index=False)\n",
    "\n",
    "    print(f\"[{crop}] Top harmful: {best_col} r={corr_val:.3f}  |  stages={stage_counts.to_dict()}  |  → {outdir}\")\n",
    "\n",
    "# ===== RUN =====\n",
    "for crop, path in IN.items():\n",
    "    try:\n",
    "        analyze_crop(crop, path)\n",
    "    except Exception as e:\n",
    "        print(f\"[{crop}] ERROR: {e}\")\n",
    "\n",
    "print(\"Done →\", OUTROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c6bb1c1f-ac34-4df8-a075-5a8bba14b96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corn stage rows: 37800 | wide rows: (4200, 155)\n",
      "Model table: (3547, 157) → Kashish_results/corn/corn_model_table1.csv\n",
      "2023 inference: (100, 155) → Kashish_results/corn/corn_inference_20231.csv\n"
     ]
    }
   ],
   "source": [
    "# corn_pipeline.py\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= CONFIG =========\n",
    "DATA_DIR = Path(\"Daily_for_transform/Daily Climate Data\")\n",
    "PATTERN  = \"PRISM_ppt_tmin_tmean_tmax_tdmean_vpdmin_vpdmax_stable_800m_*.csv\"\n",
    "OUTDIR   = Path(\"Kashish_results/corn\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "YIELD_CSV = \"crop_yield_1980-2022.csv\"   # change if different\n",
    "TARGET = \"Yield\"\n",
    "\n",
    "# ========= ASSUMED AVAILABLE (from your session) =========\n",
    "# - load_prism_all(DATA_DIR, PATTERN)\n",
    "# - build_daily(raw)  -> includes ppt_mm, t*, VPD*, ET0_proxy, P_minus_ET, HeatDay, DryDay, FrostDay, ThermalAmp, DewDep_C\n",
    "# - detect_emergence(daily)\n",
    "# - longest_streak_bool(series_of_bool) -> max consecutive True\n",
    "\n",
    "# ========= Corn phenology (DAE) =========\n",
    "CORN_STAGES = [\n",
    "    (\"VE\",   0,   10),\n",
    "    (\"V6\",  25,   30),\n",
    "    (\"VT\",  55,   59),\n",
    "    (\"R1\",  60,   70),\n",
    "    (\"R2\",  71,   80),\n",
    "    (\"R3\",  81,   90),\n",
    "    (\"R4\",  91,  105),\n",
    "    (\"R5\", 106,  125),\n",
    "    (\"R6\", 126,  160),\n",
    "]\n",
    "\n",
    "def tag_corn_stages(d: pd.DataFrame, emer_tbl: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = d.merge(emer_tbl, on=[\"Name\",\"Year\"], how=\"left\")\n",
    "    m[\"DAE\"] = (m[\"Date\"] - m[\"Emergence\"]).dt.days\n",
    "    def stage_from_dae(v):\n",
    "        if pd.isna(v): return None\n",
    "        for lab, lo, hi in CORN_STAGES:\n",
    "            if lo <= v <= hi: return lab\n",
    "        return None\n",
    "    m[\"Stage\"] = m[\"DAE\"].apply(stage_from_dae)\n",
    "    return m\n",
    "\n",
    "def corn_stage_aggregate(tagged: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = tagged.dropna(subset=[\"Stage\"]).copy()\n",
    "    g = g.sort_values([\"Name\",\"Year\",\"Stage\",\"Date\"])\n",
    "\n",
    "    core = (g.groupby([\"Name\",\"Year\",\"Stage\"], as_index=False)\n",
    "              .agg(\n",
    "                  # thermal\n",
    "                  GDD_sum=(\"GDD_day\",\"sum\"),\n",
    "                  HeatDays=(\"HeatDay\",\"sum\"),\n",
    "                  FrostDays=(\"FrostDay\",\"sum\"),\n",
    "                  Tmean_C=(\"tmean_C\",\"mean\"),\n",
    "                  Tmax_C=(\"tmax_C\",\"mean\"),\n",
    "                  ThermalAmp=(\"ThermalAmp\",\"mean\"),\n",
    "                  # water\n",
    "                  PPT_mm=(\"ppt_mm\",\"sum\"),\n",
    "                  ET0_sum=(\"ET0_proxy\",\"sum\"),\n",
    "                  P_minus_ET=(\"P_minus_ET\",\"sum\"),\n",
    "                  DryDays=(\"DryDay\",\"sum\"),\n",
    "                  HeavyRainDays=(\"HeavyRain\",\"sum\"),\n",
    "                  # VPD / humidity\n",
    "                  VPDmax_kPa=(\"vpdmax_kPa\",\"mean\"),\n",
    "                  VPDmean_kPa=(\"vpdmean_kPa\",\"mean\"),\n",
    "                  VPD_hi_days=(\"VPD_hi_day\",\"sum\"),\n",
    "                  VPD_load_sum=(\"VPD_load\",\"sum\"),\n",
    "                  DewDep_C=(\"DewDep_C\",\"mean\"),\n",
    "              ))\n",
    "\n",
    "    grp = g.groupby([\"Name\",\"Year\",\"Stage\"], observed=True)\n",
    "    streaks = (pd.DataFrame({\n",
    "        \"LongestDryStreak\": grp[\"DryDay\"].apply(longest_streak_bool),\n",
    "        \"LongestHotStreak\": grp[\"HeatDay\"].apply(longest_streak_bool),\n",
    "    }).reset_index())\n",
    "\n",
    "    out = core.merge(streaks, on=[\"Name\",\"Year\",\"Stage\"], how=\"left\")\n",
    "    out[\"EffPrecRatio_idx\"] = out[\"PPT_mm\"] / out[\"ET0_sum\"].replace(0, np.nan)\n",
    "    return out\n",
    "\n",
    "# ========= RUN: features =========\n",
    "raw   = load_prism_all(DATA_DIR, PATTERN)\n",
    "daily = build_daily(raw)\n",
    "emer  = detect_emergence(daily)\n",
    "\n",
    "corn_tagged = tag_corn_stages(daily, emer)\n",
    "corn_stage  = corn_stage_aggregate(corn_tagged)\n",
    "\n",
    "corn_tagged.to_parquet(OUTDIR/\"corn_daily_with_stages.parquet\", index=False)\n",
    "corn_stage.to_csv(OUTDIR/\"corn_stage_features_long.csv\", index=False)\n",
    "\n",
    "# wide table\n",
    "wide = (corn_stage.pivot_table(\n",
    "            index=[\"Name\",\"Year\"], columns=\"Stage\",\n",
    "            values=[\"PPT_mm\",\"ET0_sum\",\"P_minus_ET\",\"EffPrecRatio_idx\",\n",
    "                    \"GDD_sum\",\"HeatDays\",\"FrostDays\",\"LongestDryStreak\",\"LongestHotStreak\",\n",
    "                    \"Tmean_C\",\"Tmax_C\",\"ThermalAmp\",\n",
    "                    \"VPDmax_kPa\",\"VPDmean_kPa\",\"VPD_hi_days\",\"VPD_load_sum\",\"DewDep_C\"],\n",
    "            aggfunc=\"first\")\n",
    "        .reset_index())\n",
    "wide.columns = [\"*\".join(c).strip(\"*\") for c in wide.columns.to_flat_index()]\n",
    "wide.to_csv(OUTDIR/\"corn_stage_features_wide.csv\", index=False)\n",
    "\n",
    "# ========= MERGE YIELD ≤2022 to build model table =========\n",
    "yld = pd.read_csv(YIELD_CSV)\n",
    "\n",
    "# normalize county name column and yield\n",
    "def norm_county_col(df):\n",
    "    for c in [\"County\",\"COUNTY\",\"Name\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.rename(columns={c:\"Name\"})\n",
    "            break\n",
    "    df[\"Name\"] = df[\"Name\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "yld = norm_county_col(yld)\n",
    "# corn grain yield; use YIELD_LBS_PER_ACRE (already present in your file)\n",
    "is_corn = yld[\"Commodity\"].str.upper().str.contains(\"^CORN$\")\n",
    "grain   = yld[\"Data Item\"].str.upper().str.contains(\"GRAIN - YIELD\")\n",
    "yld_corn = (yld.loc[is_corn & grain, [\"Year\",\"Name\",\"Commodity\",\"YIELD_LBS_PER_ACRE\"]]\n",
    "              .rename(columns={\"YIELD_LBS_PER_ACRE\": TARGET}))\n",
    "yld_corn[TARGET] = pd.to_numeric(yld_corn[TARGET], errors=\"coerce\")\n",
    "\n",
    "wide2 = norm_county_col(wide)\n",
    "model_tbl = (yld_corn.merge(wide2[wide2[\"Year\"]<=2022], on=[\"Name\",\"Year\"], how=\"inner\")\n",
    "                     .dropna(subset=[TARGET]))\n",
    "model_tbl.to_csv(OUTDIR/\"corn_model_table1.csv\", index=False)\n",
    "\n",
    "# ========= INFERENCE SET FOR 2023 =========\n",
    "infer_2023 = wide2[wide2[\"Year\"]==2023].copy()\n",
    "# keep IDs + features only\n",
    "id_cols = [\"Name\",\"Year\"]\n",
    "feat_cols = [c for c in infer_2023.columns if c not in id_cols]\n",
    "infer_2023 = infer_2023[id_cols + feat_cols]\n",
    "infer_2023.to_csv(OUTDIR/\"corn_inference_20231.csv\", index=False)\n",
    "\n",
    "print(\"Corn stage rows:\", len(corn_stage), \"| wide rows:\", wide.shape)\n",
    "print(\"Model table:\", model_tbl.shape, \"→\", OUTDIR/\"corn_model_table1.csv\")\n",
    "print(\"2023 inference:\", infer_2023.shape, \"→\", OUTDIR/\"corn_inference_20231.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0aae247d-73ce-4627-b157-235f353db86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: Kashish_results/corn/corn_inference_20231_colsfixed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "src = Path(\"Kashish_results/corn/corn_inference_20231.csv\")\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# replace '*' with '_' in all column names\n",
    "df.columns = [c.replace(\"*\", \"_\") for c in df.columns]\n",
    "\n",
    "# save (new file to be safe)\n",
    "dst = src.with_name(src.stem + \"_colsfixed.csv\")\n",
    "df.to_csv(dst, index=False)\n",
    "print(\"Wrote:\", dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa84c5-fad3-4042-b9b9-9e56987c1813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
